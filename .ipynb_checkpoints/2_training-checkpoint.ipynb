{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "eBdawtOnAT5Y",
    "outputId": "4db231c2-69ba-4c78-f169-9f3b085a2197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "6PMMB5EsutnJ",
    "outputId": "280efd69-8894-4dd6-cfcd-e6041566ab37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json\t      list_eval_partition.csv\n",
      "celeba-dataset.zip    list_landmarks_align_celeba.csv\n",
      "iu.jpeg\t\t      mmod_human_face_detector.dat\n",
      "kaggle.json\t      sample_data\n",
      "list_attr_celeba.csv  train_data.pkl\n",
      "list_bbox_celeba.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cxQBSKKssAV9",
    "outputId": "d7594185-9589-4898-b56f-0336b387c8d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "ksNpYRdHAtbO",
    "outputId": "cce67920-ac4c-43a6-b88f-819e673613ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.4)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "h2OnZpcYAxLT",
    "outputId": "5855d21e-3175-462a-9469-0fdd9acb3958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading celeba-dataset.zip to /content\n",
      "100% 1.21G/1.21G [00:25<00:00, 82.0MB/s]\n",
      "100% 1.21G/1.21G [00:25<00:00, 51.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "JaE_p3YZMjEt",
    "outputId": "530e209f-fb9c-4ba1-c114-745b90e5d365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celeba-dataset.zip  kaggle.json  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iViCCczMBaqZ"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"/content/celeba-dataset.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"/content\")\n",
    "    \n",
    "with zipfile.ZipFile(\"/content/img_align_celeba.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "bPwDZEh-_OQ1",
    "outputId": "7d7d1610-f70c-459b-9fac-67461867a163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json\t      list_bbox_celeba.csv\n",
      "celeba-dataset.zip    list_eval_partition.csv\n",
      "img_align_celeba      list_landmarks_align_celeba.csv\n",
      "img_align_celeba.zip  mmod_human_face_detector.dat\n",
      "iu.jpeg\t\t      sample_data\n",
      "kaggle.json\t      train_data.pkl\n",
      "list_attr_celeba.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1989
    },
    "colab_type": "code",
    "id": "h3PUoMOWCHJB",
    "outputId": "2f01ba6c-91ae-4620-9c14-b51a984cb07c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>5_o_Clock_Shadow</th>\n",
       "      <th>Arched_Eyebrows</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Bags_Under_Eyes</th>\n",
       "      <th>Bald</th>\n",
       "      <th>Bangs</th>\n",
       "      <th>Big_Lips</th>\n",
       "      <th>Big_Nose</th>\n",
       "      <th>Black_Hair</th>\n",
       "      <th>Blond_Hair</th>\n",
       "      <th>Blurry</th>\n",
       "      <th>Brown_Hair</th>\n",
       "      <th>Bushy_Eyebrows</th>\n",
       "      <th>Chubby</th>\n",
       "      <th>Double_Chin</th>\n",
       "      <th>Eyeglasses</th>\n",
       "      <th>Goatee</th>\n",
       "      <th>Gray_Hair</th>\n",
       "      <th>Heavy_Makeup</th>\n",
       "      <th>High_Cheekbones</th>\n",
       "      <th>Male</th>\n",
       "      <th>Mouth_Slightly_Open</th>\n",
       "      <th>Mustache</th>\n",
       "      <th>Narrow_Eyes</th>\n",
       "      <th>No_Beard</th>\n",
       "      <th>Oval_Face</th>\n",
       "      <th>Pale_Skin</th>\n",
       "      <th>Pointy_Nose</th>\n",
       "      <th>Receding_Hairline</th>\n",
       "      <th>Rosy_Cheeks</th>\n",
       "      <th>Sideburns</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Straight_Hair</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>Wearing_Earrings</th>\n",
       "      <th>Wearing_Hat</th>\n",
       "      <th>Wearing_Lipstick</th>\n",
       "      <th>Wearing_Necklace</th>\n",
       "      <th>Wearing_Necktie</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000006.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000007.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000008.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000009.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000010.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>000011.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000012.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000013.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000014.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>000015.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000016.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000017.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000018.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000019.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>000020.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>000021.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000022.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000023.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>000024.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>000025.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>000026.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000027.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000028.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000029.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>000030.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202569</th>\n",
       "      <td>202570.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202570</th>\n",
       "      <td>202571.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202571</th>\n",
       "      <td>202572.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202572</th>\n",
       "      <td>202573.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202573</th>\n",
       "      <td>202574.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202574</th>\n",
       "      <td>202575.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202575</th>\n",
       "      <td>202576.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202576</th>\n",
       "      <td>202577.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202577</th>\n",
       "      <td>202578.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202578</th>\n",
       "      <td>202579.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202579</th>\n",
       "      <td>202580.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202580</th>\n",
       "      <td>202581.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202581</th>\n",
       "      <td>202582.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202582</th>\n",
       "      <td>202583.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202583</th>\n",
       "      <td>202584.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202584</th>\n",
       "      <td>202585.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202585</th>\n",
       "      <td>202586.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202586</th>\n",
       "      <td>202587.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202587</th>\n",
       "      <td>202588.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202588</th>\n",
       "      <td>202589.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202589</th>\n",
       "      <td>202590.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202590</th>\n",
       "      <td>202591.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202591</th>\n",
       "      <td>202592.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202592</th>\n",
       "      <td>202593.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202593</th>\n",
       "      <td>202594.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202594</th>\n",
       "      <td>202595.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202595</th>\n",
       "      <td>202596.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202596</th>\n",
       "      <td>202597.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202597</th>\n",
       "      <td>202598.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202598</th>\n",
       "      <td>202599.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202599 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  5_o_Clock_Shadow  ...  Wearing_Necktie  Young\n",
       "0       000001.jpg                -1  ...               -1      1\n",
       "1       000002.jpg                -1  ...               -1      1\n",
       "2       000003.jpg                -1  ...               -1      1\n",
       "3       000004.jpg                -1  ...               -1      1\n",
       "4       000005.jpg                -1  ...               -1      1\n",
       "5       000006.jpg                -1  ...               -1      1\n",
       "6       000007.jpg                 1  ...               -1      1\n",
       "7       000008.jpg                 1  ...               -1      1\n",
       "8       000009.jpg                -1  ...               -1      1\n",
       "9       000010.jpg                -1  ...               -1      1\n",
       "10      000011.jpg                -1  ...               -1      1\n",
       "11      000012.jpg                -1  ...               -1      1\n",
       "12      000013.jpg                -1  ...               -1      1\n",
       "13      000014.jpg                -1  ...               -1      1\n",
       "14      000015.jpg                 1  ...                1     -1\n",
       "15      000016.jpg                 1  ...               -1      1\n",
       "16      000017.jpg                -1  ...               -1      1\n",
       "17      000018.jpg                -1  ...               -1     -1\n",
       "18      000019.jpg                -1  ...               -1      1\n",
       "19      000020.jpg                -1  ...               -1      1\n",
       "20      000021.jpg                -1  ...                1     -1\n",
       "21      000022.jpg                -1  ...               -1      1\n",
       "22      000023.jpg                 1  ...               -1      1\n",
       "23      000024.jpg                -1  ...               -1      1\n",
       "24      000025.jpg                 1  ...               -1      1\n",
       "25      000026.jpg                -1  ...               -1      1\n",
       "26      000027.jpg                -1  ...               -1      1\n",
       "27      000028.jpg                -1  ...               -1      1\n",
       "28      000029.jpg                -1  ...               -1      1\n",
       "29      000030.jpg                -1  ...               -1     -1\n",
       "...            ...               ...  ...              ...    ...\n",
       "202569  202570.jpg                -1  ...               -1      1\n",
       "202570  202571.jpg                -1  ...               -1      1\n",
       "202571  202572.jpg                -1  ...               -1      1\n",
       "202572  202573.jpg                -1  ...               -1     -1\n",
       "202573  202574.jpg                -1  ...               -1      1\n",
       "202574  202575.jpg                -1  ...               -1      1\n",
       "202575  202576.jpg                -1  ...               -1      1\n",
       "202576  202577.jpg                -1  ...               -1      1\n",
       "202577  202578.jpg                -1  ...               -1      1\n",
       "202578  202579.jpg                -1  ...               -1      1\n",
       "202579  202580.jpg                -1  ...               -1      1\n",
       "202580  202581.jpg                 1  ...                1     -1\n",
       "202581  202582.jpg                -1  ...               -1      1\n",
       "202582  202583.jpg                -1  ...               -1      1\n",
       "202583  202584.jpg                -1  ...               -1      1\n",
       "202584  202585.jpg                -1  ...               -1      1\n",
       "202585  202586.jpg                -1  ...                1     -1\n",
       "202586  202587.jpg                -1  ...               -1      1\n",
       "202587  202588.jpg                -1  ...                1      1\n",
       "202588  202589.jpg                 1  ...               -1      1\n",
       "202589  202590.jpg                -1  ...               -1     -1\n",
       "202590  202591.jpg                -1  ...               -1      1\n",
       "202591  202592.jpg                -1  ...               -1      1\n",
       "202592  202593.jpg                -1  ...               -1      1\n",
       "202593  202594.jpg                -1  ...               -1      1\n",
       "202594  202595.jpg                -1  ...               -1      1\n",
       "202595  202596.jpg                -1  ...               -1      1\n",
       "202596  202597.jpg                -1  ...               -1      1\n",
       "202597  202598.jpg                -1  ...               -1      1\n",
       "202598  202599.jpg                -1  ...               -1      1\n",
       "\n",
       "[202599 rows x 41 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "data=pd.read_csv('/content/list_attr_celeba.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWyCXSS5CKkN"
   },
   "outputs": [],
   "source": [
    "col_list=data.columns.tolist()\n",
    "col_list.remove('image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCf6fKfSCN8s"
   },
   "outputs": [],
   "source": [
    "#!pip install -U -q PyDrive로 PyDrive를 설치하고 기타 필요한 라이브러리들을 import한다 \n",
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8m0BY3-CVMa"
   },
   "outputs": [],
   "source": [
    "#PyDrive 설치가 끝났으면 클라이언트를 인증하고 생성한다. 구글 계정으로 로그인하여 인증번호를 받아 입력하면 인증이 완료된다.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "#구글 계정 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "WO1ebhlLCpoi",
    "outputId": "de07d9f8-dbf4-4808-af74-91efda917b81"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-792ed6ea4ec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'1oXCCSdw4GeSLQ8gyv3U2z3Bh3O-einYh'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
     ]
    }
   ],
   "source": [
    "#train_data.pkl의 id값을 이용해 구글 드라이브로부터 pkl파일을 다운받는다. (자세한 내용 피피티 참조)\n",
    "download = drive.CreateFile({'id': '1oXCCSdw4GeSLQ8gyv3U2z3Bh3O-einYh'})\n",
    "download.GetContentFile('train_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "mr_CBAw5Ed5k",
    "outputId": "d9ee1773-d5d7-4637-acaa-3cf95cecf09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json\t      kaggle.json\t       list_landmarks_align_celeba.csv\n",
      "celeba-dataset.zip    list_attr_celeba.csv     sample_data\n",
      "img_align_celeba      list_bbox_celeba.csv     train_data.pkl\n",
      "img_align_celeba.zip  list_eval_partition.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybqTUZNXCzvh"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_data.pkl','rb') as f: \n",
    "    train_files,train_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b1GYf4HLC6W4",
    "outputId": "b2e38ed5-f589-4af6-edc3-0c21c1861d29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "id": "qetDA_N0C-Kf",
    "outputId": "cd422fe3-fcab-4965-ab1d-6ae9e46fc8dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img_align_celeba/010005.jpg',\n",
       " 'img_align_celeba/010000.jpg',\n",
       " 'img_align_celeba/010033.jpg',\n",
       " 'img_align_celeba/010001.jpg',\n",
       " 'img_align_celeba/010038.jpg',\n",
       " 'img_align_celeba/010002.jpg',\n",
       " 'img_align_celeba/010039.jpg',\n",
       " 'img_align_celeba/010003.jpg',\n",
       " 'img_align_celeba/010046.jpg',\n",
       " 'img_align_celeba/010004.jpg',\n",
       " 'img_align_celeba/010052.jpg',\n",
       " 'img_align_celeba/010006.jpg',\n",
       " 'img_align_celeba/010056.jpg',\n",
       " 'img_align_celeba/010007.jpg',\n",
       " 'img_align_celeba/010058.jpg',\n",
       " 'img_align_celeba/010008.jpg',\n",
       " 'img_align_celeba/010063.jpg',\n",
       " 'img_align_celeba/010009.jpg',\n",
       " 'img_align_celeba/010069.jpg',\n",
       " 'img_align_celeba/010010.jpg',\n",
       " 'img_align_celeba/010072.jpg',\n",
       " 'img_align_celeba/010011.jpg',\n",
       " 'img_align_celeba/010083.jpg',\n",
       " 'img_align_celeba/010012.jpg',\n",
       " 'img_align_celeba/010087.jpg',\n",
       " 'img_align_celeba/010013.jpg',\n",
       " 'img_align_celeba/010097.jpg',\n",
       " 'img_align_celeba/010014.jpg',\n",
       " 'img_align_celeba/010100.jpg',\n",
       " 'img_align_celeba/010015.jpg',\n",
       " 'img_align_celeba/010102.jpg',\n",
       " 'img_align_celeba/010016.jpg',\n",
       " 'img_align_celeba/010117.jpg',\n",
       " 'img_align_celeba/010017.jpg',\n",
       " 'img_align_celeba/010133.jpg',\n",
       " 'img_align_celeba/010018.jpg',\n",
       " 'img_align_celeba/010139.jpg',\n",
       " 'img_align_celeba/010019.jpg',\n",
       " 'img_align_celeba/010141.jpg',\n",
       " 'img_align_celeba/010020.jpg',\n",
       " 'img_align_celeba/010146.jpg',\n",
       " 'img_align_celeba/010021.jpg',\n",
       " 'img_align_celeba/010147.jpg',\n",
       " 'img_align_celeba/010022.jpg',\n",
       " 'img_align_celeba/010157.jpg',\n",
       " 'img_align_celeba/010023.jpg',\n",
       " 'img_align_celeba/010168.jpg',\n",
       " 'img_align_celeba/010024.jpg',\n",
       " 'img_align_celeba/010176.jpg',\n",
       " 'img_align_celeba/010025.jpg',\n",
       " 'img_align_celeba/010198.jpg',\n",
       " 'img_align_celeba/010026.jpg',\n",
       " 'img_align_celeba/010226.jpg',\n",
       " 'img_align_celeba/010027.jpg',\n",
       " 'img_align_celeba/010232.jpg',\n",
       " 'img_align_celeba/010028.jpg',\n",
       " 'img_align_celeba/010246.jpg',\n",
       " 'img_align_celeba/010029.jpg',\n",
       " 'img_align_celeba/010253.jpg',\n",
       " 'img_align_celeba/010030.jpg',\n",
       " 'img_align_celeba/010260.jpg',\n",
       " 'img_align_celeba/010031.jpg',\n",
       " 'img_align_celeba/010262.jpg',\n",
       " 'img_align_celeba/010032.jpg',\n",
       " 'img_align_celeba/010266.jpg',\n",
       " 'img_align_celeba/010034.jpg',\n",
       " 'img_align_celeba/010279.jpg',\n",
       " 'img_align_celeba/010035.jpg',\n",
       " 'img_align_celeba/010281.jpg',\n",
       " 'img_align_celeba/010036.jpg',\n",
       " 'img_align_celeba/010286.jpg',\n",
       " 'img_align_celeba/010037.jpg',\n",
       " 'img_align_celeba/010309.jpg',\n",
       " 'img_align_celeba/010040.jpg',\n",
       " 'img_align_celeba/010315.jpg',\n",
       " 'img_align_celeba/010041.jpg',\n",
       " 'img_align_celeba/010321.jpg',\n",
       " 'img_align_celeba/010042.jpg',\n",
       " 'img_align_celeba/010328.jpg',\n",
       " 'img_align_celeba/010043.jpg',\n",
       " 'img_align_celeba/010333.jpg',\n",
       " 'img_align_celeba/010044.jpg',\n",
       " 'img_align_celeba/010336.jpg',\n",
       " 'img_align_celeba/010045.jpg',\n",
       " 'img_align_celeba/010375.jpg',\n",
       " 'img_align_celeba/010047.jpg',\n",
       " 'img_align_celeba/010390.jpg',\n",
       " 'img_align_celeba/010048.jpg',\n",
       " 'img_align_celeba/010396.jpg',\n",
       " 'img_align_celeba/010049.jpg',\n",
       " 'img_align_celeba/010411.jpg',\n",
       " 'img_align_celeba/010050.jpg',\n",
       " 'img_align_celeba/010418.jpg',\n",
       " 'img_align_celeba/010051.jpg',\n",
       " 'img_align_celeba/010437.jpg',\n",
       " 'img_align_celeba/010053.jpg',\n",
       " 'img_align_celeba/010438.jpg',\n",
       " 'img_align_celeba/010054.jpg',\n",
       " 'img_align_celeba/010446.jpg',\n",
       " 'img_align_celeba/010055.jpg',\n",
       " 'img_align_celeba/010449.jpg',\n",
       " 'img_align_celeba/010057.jpg',\n",
       " 'img_align_celeba/010458.jpg',\n",
       " 'img_align_celeba/010059.jpg',\n",
       " 'img_align_celeba/010476.jpg',\n",
       " 'img_align_celeba/010060.jpg',\n",
       " 'img_align_celeba/010485.jpg',\n",
       " 'img_align_celeba/010061.jpg',\n",
       " 'img_align_celeba/010490.jpg',\n",
       " 'img_align_celeba/010062.jpg',\n",
       " 'img_align_celeba/010491.jpg',\n",
       " 'img_align_celeba/010064.jpg',\n",
       " 'img_align_celeba/010493.jpg',\n",
       " 'img_align_celeba/010065.jpg',\n",
       " 'img_align_celeba/010505.jpg',\n",
       " 'img_align_celeba/010066.jpg',\n",
       " 'img_align_celeba/010506.jpg',\n",
       " 'img_align_celeba/010067.jpg',\n",
       " 'img_align_celeba/010508.jpg',\n",
       " 'img_align_celeba/010068.jpg',\n",
       " 'img_align_celeba/010510.jpg',\n",
       " 'img_align_celeba/010070.jpg',\n",
       " 'img_align_celeba/010518.jpg',\n",
       " 'img_align_celeba/010071.jpg',\n",
       " 'img_align_celeba/010537.jpg',\n",
       " 'img_align_celeba/010073.jpg',\n",
       " 'img_align_celeba/010549.jpg',\n",
       " 'img_align_celeba/010074.jpg',\n",
       " 'img_align_celeba/010550.jpg',\n",
       " 'img_align_celeba/010075.jpg',\n",
       " 'img_align_celeba/010580.jpg',\n",
       " 'img_align_celeba/010076.jpg',\n",
       " 'img_align_celeba/010581.jpg',\n",
       " 'img_align_celeba/010077.jpg',\n",
       " 'img_align_celeba/010591.jpg',\n",
       " 'img_align_celeba/010078.jpg',\n",
       " 'img_align_celeba/010597.jpg',\n",
       " 'img_align_celeba/010079.jpg',\n",
       " 'img_align_celeba/010601.jpg',\n",
       " 'img_align_celeba/010080.jpg',\n",
       " 'img_align_celeba/010606.jpg',\n",
       " 'img_align_celeba/010081.jpg',\n",
       " 'img_align_celeba/010625.jpg',\n",
       " 'img_align_celeba/010082.jpg',\n",
       " 'img_align_celeba/010631.jpg',\n",
       " 'img_align_celeba/010084.jpg',\n",
       " 'img_align_celeba/010632.jpg',\n",
       " 'img_align_celeba/010085.jpg',\n",
       " 'img_align_celeba/010641.jpg',\n",
       " 'img_align_celeba/010086.jpg',\n",
       " 'img_align_celeba/010646.jpg',\n",
       " 'img_align_celeba/010088.jpg',\n",
       " 'img_align_celeba/010652.jpg',\n",
       " 'img_align_celeba/010089.jpg',\n",
       " 'img_align_celeba/010653.jpg',\n",
       " 'img_align_celeba/010090.jpg',\n",
       " 'img_align_celeba/010658.jpg',\n",
       " 'img_align_celeba/010091.jpg',\n",
       " 'img_align_celeba/010663.jpg',\n",
       " 'img_align_celeba/010092.jpg',\n",
       " 'img_align_celeba/010677.jpg',\n",
       " 'img_align_celeba/010093.jpg',\n",
       " 'img_align_celeba/010709.jpg',\n",
       " 'img_align_celeba/010094.jpg',\n",
       " 'img_align_celeba/010717.jpg',\n",
       " 'img_align_celeba/010095.jpg',\n",
       " 'img_align_celeba/010723.jpg',\n",
       " 'img_align_celeba/010096.jpg',\n",
       " 'img_align_celeba/010725.jpg',\n",
       " 'img_align_celeba/010098.jpg',\n",
       " 'img_align_celeba/010727.jpg',\n",
       " 'img_align_celeba/010099.jpg',\n",
       " 'img_align_celeba/010733.jpg',\n",
       " 'img_align_celeba/010101.jpg',\n",
       " 'img_align_celeba/010738.jpg',\n",
       " 'img_align_celeba/010103.jpg',\n",
       " 'img_align_celeba/010747.jpg',\n",
       " 'img_align_celeba/010104.jpg',\n",
       " 'img_align_celeba/010757.jpg',\n",
       " 'img_align_celeba/010105.jpg',\n",
       " 'img_align_celeba/010771.jpg',\n",
       " 'img_align_celeba/010106.jpg',\n",
       " 'img_align_celeba/010778.jpg',\n",
       " 'img_align_celeba/010107.jpg',\n",
       " 'img_align_celeba/010814.jpg',\n",
       " 'img_align_celeba/010108.jpg',\n",
       " 'img_align_celeba/010818.jpg',\n",
       " 'img_align_celeba/010109.jpg',\n",
       " 'img_align_celeba/010832.jpg',\n",
       " 'img_align_celeba/010110.jpg',\n",
       " 'img_align_celeba/010851.jpg',\n",
       " 'img_align_celeba/010111.jpg',\n",
       " 'img_align_celeba/010855.jpg',\n",
       " 'img_align_celeba/010112.jpg',\n",
       " 'img_align_celeba/010858.jpg',\n",
       " 'img_align_celeba/010113.jpg',\n",
       " 'img_align_celeba/010938.jpg',\n",
       " 'img_align_celeba/010114.jpg',\n",
       " 'img_align_celeba/010947.jpg',\n",
       " 'img_align_celeba/010115.jpg',\n",
       " 'img_align_celeba/010952.jpg',\n",
       " 'img_align_celeba/010116.jpg',\n",
       " 'img_align_celeba/010973.jpg',\n",
       " 'img_align_celeba/010118.jpg',\n",
       " 'img_align_celeba/010974.jpg',\n",
       " 'img_align_celeba/010119.jpg',\n",
       " 'img_align_celeba/010979.jpg',\n",
       " 'img_align_celeba/010120.jpg',\n",
       " 'img_align_celeba/010980.jpg',\n",
       " 'img_align_celeba/010121.jpg',\n",
       " 'img_align_celeba/010985.jpg',\n",
       " 'img_align_celeba/010122.jpg',\n",
       " 'img_align_celeba/010988.jpg',\n",
       " 'img_align_celeba/010123.jpg',\n",
       " 'img_align_celeba/010991.jpg',\n",
       " 'img_align_celeba/010124.jpg',\n",
       " 'img_align_celeba/010993.jpg',\n",
       " 'img_align_celeba/010125.jpg',\n",
       " 'img_align_celeba/010996.jpg',\n",
       " 'img_align_celeba/010126.jpg',\n",
       " 'img_align_celeba/011002.jpg',\n",
       " 'img_align_celeba/010127.jpg',\n",
       " 'img_align_celeba/011004.jpg',\n",
       " 'img_align_celeba/010128.jpg',\n",
       " 'img_align_celeba/011006.jpg',\n",
       " 'img_align_celeba/010129.jpg',\n",
       " 'img_align_celeba/011008.jpg',\n",
       " 'img_align_celeba/010130.jpg',\n",
       " 'img_align_celeba/011022.jpg',\n",
       " 'img_align_celeba/010131.jpg',\n",
       " 'img_align_celeba/011027.jpg',\n",
       " 'img_align_celeba/010132.jpg',\n",
       " 'img_align_celeba/011034.jpg',\n",
       " 'img_align_celeba/010134.jpg',\n",
       " 'img_align_celeba/011039.jpg',\n",
       " 'img_align_celeba/010135.jpg',\n",
       " 'img_align_celeba/011048.jpg',\n",
       " 'img_align_celeba/010136.jpg',\n",
       " 'img_align_celeba/011052.jpg',\n",
       " 'img_align_celeba/010137.jpg',\n",
       " 'img_align_celeba/011068.jpg',\n",
       " 'img_align_celeba/010138.jpg',\n",
       " 'img_align_celeba/011083.jpg',\n",
       " 'img_align_celeba/010140.jpg',\n",
       " 'img_align_celeba/011093.jpg',\n",
       " 'img_align_celeba/010142.jpg',\n",
       " 'img_align_celeba/011106.jpg',\n",
       " 'img_align_celeba/010143.jpg',\n",
       " 'img_align_celeba/011107.jpg',\n",
       " 'img_align_celeba/010144.jpg',\n",
       " 'img_align_celeba/011121.jpg',\n",
       " 'img_align_celeba/010145.jpg',\n",
       " 'img_align_celeba/011122.jpg',\n",
       " 'img_align_celeba/010148.jpg',\n",
       " 'img_align_celeba/011133.jpg',\n",
       " 'img_align_celeba/010149.jpg',\n",
       " 'img_align_celeba/011141.jpg',\n",
       " 'img_align_celeba/010150.jpg',\n",
       " 'img_align_celeba/011145.jpg',\n",
       " 'img_align_celeba/010151.jpg',\n",
       " 'img_align_celeba/011159.jpg',\n",
       " 'img_align_celeba/010152.jpg',\n",
       " 'img_align_celeba/011167.jpg',\n",
       " 'img_align_celeba/010153.jpg',\n",
       " 'img_align_celeba/011174.jpg',\n",
       " 'img_align_celeba/010154.jpg',\n",
       " 'img_align_celeba/011183.jpg',\n",
       " 'img_align_celeba/010155.jpg',\n",
       " 'img_align_celeba/011196.jpg',\n",
       " 'img_align_celeba/010156.jpg',\n",
       " 'img_align_celeba/011200.jpg',\n",
       " 'img_align_celeba/010158.jpg',\n",
       " 'img_align_celeba/011201.jpg',\n",
       " 'img_align_celeba/010159.jpg',\n",
       " 'img_align_celeba/011202.jpg',\n",
       " 'img_align_celeba/010160.jpg',\n",
       " 'img_align_celeba/011217.jpg',\n",
       " 'img_align_celeba/010161.jpg',\n",
       " 'img_align_celeba/011222.jpg',\n",
       " 'img_align_celeba/010162.jpg',\n",
       " 'img_align_celeba/011224.jpg',\n",
       " 'img_align_celeba/010163.jpg',\n",
       " 'img_align_celeba/011227.jpg',\n",
       " 'img_align_celeba/010164.jpg',\n",
       " 'img_align_celeba/011228.jpg',\n",
       " 'img_align_celeba/010165.jpg',\n",
       " 'img_align_celeba/011252.jpg',\n",
       " 'img_align_celeba/010166.jpg',\n",
       " 'img_align_celeba/011256.jpg',\n",
       " 'img_align_celeba/010167.jpg',\n",
       " 'img_align_celeba/011266.jpg',\n",
       " 'img_align_celeba/010169.jpg',\n",
       " 'img_align_celeba/011278.jpg',\n",
       " 'img_align_celeba/010170.jpg',\n",
       " 'img_align_celeba/011279.jpg',\n",
       " 'img_align_celeba/010171.jpg',\n",
       " 'img_align_celeba/011281.jpg',\n",
       " 'img_align_celeba/010172.jpg',\n",
       " 'img_align_celeba/011284.jpg',\n",
       " 'img_align_celeba/010173.jpg',\n",
       " 'img_align_celeba/011290.jpg',\n",
       " 'img_align_celeba/010174.jpg',\n",
       " 'img_align_celeba/011304.jpg',\n",
       " 'img_align_celeba/010175.jpg',\n",
       " 'img_align_celeba/011314.jpg',\n",
       " 'img_align_celeba/010177.jpg',\n",
       " 'img_align_celeba/011316.jpg',\n",
       " 'img_align_celeba/010178.jpg',\n",
       " 'img_align_celeba/011320.jpg',\n",
       " 'img_align_celeba/010179.jpg',\n",
       " 'img_align_celeba/011325.jpg',\n",
       " 'img_align_celeba/010180.jpg',\n",
       " 'img_align_celeba/011337.jpg',\n",
       " 'img_align_celeba/010181.jpg',\n",
       " 'img_align_celeba/011338.jpg',\n",
       " 'img_align_celeba/010182.jpg',\n",
       " 'img_align_celeba/011341.jpg',\n",
       " 'img_align_celeba/010183.jpg',\n",
       " 'img_align_celeba/011344.jpg',\n",
       " 'img_align_celeba/010184.jpg',\n",
       " 'img_align_celeba/011346.jpg',\n",
       " 'img_align_celeba/010185.jpg',\n",
       " 'img_align_celeba/011352.jpg',\n",
       " 'img_align_celeba/010186.jpg',\n",
       " 'img_align_celeba/011357.jpg',\n",
       " 'img_align_celeba/010187.jpg',\n",
       " 'img_align_celeba/011358.jpg',\n",
       " 'img_align_celeba/010188.jpg',\n",
       " 'img_align_celeba/011377.jpg',\n",
       " 'img_align_celeba/010189.jpg',\n",
       " 'img_align_celeba/011381.jpg',\n",
       " 'img_align_celeba/010190.jpg',\n",
       " 'img_align_celeba/011389.jpg',\n",
       " 'img_align_celeba/010191.jpg',\n",
       " 'img_align_celeba/011392.jpg',\n",
       " 'img_align_celeba/010192.jpg',\n",
       " 'img_align_celeba/011398.jpg',\n",
       " 'img_align_celeba/010193.jpg',\n",
       " 'img_align_celeba/011403.jpg',\n",
       " 'img_align_celeba/010194.jpg',\n",
       " 'img_align_celeba/011419.jpg',\n",
       " 'img_align_celeba/010195.jpg',\n",
       " 'img_align_celeba/011420.jpg',\n",
       " 'img_align_celeba/010196.jpg',\n",
       " 'img_align_celeba/011423.jpg',\n",
       " 'img_align_celeba/010197.jpg',\n",
       " 'img_align_celeba/011432.jpg',\n",
       " 'img_align_celeba/010199.jpg',\n",
       " 'img_align_celeba/011435.jpg',\n",
       " 'img_align_celeba/010200.jpg',\n",
       " 'img_align_celeba/011447.jpg',\n",
       " 'img_align_celeba/010201.jpg',\n",
       " 'img_align_celeba/011472.jpg',\n",
       " 'img_align_celeba/010202.jpg',\n",
       " 'img_align_celeba/011511.jpg',\n",
       " 'img_align_celeba/010203.jpg',\n",
       " 'img_align_celeba/011514.jpg',\n",
       " 'img_align_celeba/010204.jpg',\n",
       " 'img_align_celeba/011516.jpg',\n",
       " 'img_align_celeba/010205.jpg',\n",
       " 'img_align_celeba/011528.jpg',\n",
       " 'img_align_celeba/010206.jpg',\n",
       " 'img_align_celeba/011532.jpg',\n",
       " 'img_align_celeba/010207.jpg',\n",
       " 'img_align_celeba/011533.jpg',\n",
       " 'img_align_celeba/010208.jpg',\n",
       " 'img_align_celeba/011535.jpg',\n",
       " 'img_align_celeba/010209.jpg',\n",
       " 'img_align_celeba/011553.jpg',\n",
       " 'img_align_celeba/010210.jpg',\n",
       " 'img_align_celeba/011561.jpg',\n",
       " 'img_align_celeba/010211.jpg',\n",
       " 'img_align_celeba/011569.jpg',\n",
       " 'img_align_celeba/010212.jpg',\n",
       " 'img_align_celeba/011578.jpg',\n",
       " 'img_align_celeba/010213.jpg',\n",
       " 'img_align_celeba/011604.jpg',\n",
       " 'img_align_celeba/010214.jpg',\n",
       " 'img_align_celeba/011608.jpg',\n",
       " 'img_align_celeba/010215.jpg',\n",
       " 'img_align_celeba/011615.jpg',\n",
       " 'img_align_celeba/010216.jpg',\n",
       " 'img_align_celeba/011622.jpg',\n",
       " 'img_align_celeba/010217.jpg',\n",
       " 'img_align_celeba/011629.jpg',\n",
       " 'img_align_celeba/010218.jpg',\n",
       " 'img_align_celeba/011631.jpg',\n",
       " 'img_align_celeba/010219.jpg',\n",
       " 'img_align_celeba/011632.jpg',\n",
       " 'img_align_celeba/010220.jpg',\n",
       " 'img_align_celeba/011639.jpg',\n",
       " 'img_align_celeba/010221.jpg',\n",
       " 'img_align_celeba/011640.jpg',\n",
       " 'img_align_celeba/010222.jpg',\n",
       " 'img_align_celeba/011643.jpg',\n",
       " 'img_align_celeba/010223.jpg',\n",
       " 'img_align_celeba/011645.jpg',\n",
       " 'img_align_celeba/010224.jpg',\n",
       " 'img_align_celeba/011657.jpg',\n",
       " 'img_align_celeba/010225.jpg',\n",
       " 'img_align_celeba/011662.jpg',\n",
       " 'img_align_celeba/010227.jpg',\n",
       " 'img_align_celeba/011666.jpg',\n",
       " 'img_align_celeba/010228.jpg',\n",
       " 'img_align_celeba/011667.jpg',\n",
       " 'img_align_celeba/010229.jpg',\n",
       " 'img_align_celeba/011678.jpg',\n",
       " 'img_align_celeba/010230.jpg',\n",
       " 'img_align_celeba/011706.jpg',\n",
       " 'img_align_celeba/010231.jpg',\n",
       " 'img_align_celeba/011708.jpg',\n",
       " 'img_align_celeba/010233.jpg',\n",
       " 'img_align_celeba/011717.jpg',\n",
       " 'img_align_celeba/010234.jpg',\n",
       " 'img_align_celeba/011733.jpg',\n",
       " 'img_align_celeba/010235.jpg',\n",
       " 'img_align_celeba/011741.jpg',\n",
       " 'img_align_celeba/010236.jpg',\n",
       " 'img_align_celeba/011758.jpg',\n",
       " 'img_align_celeba/010237.jpg',\n",
       " 'img_align_celeba/011761.jpg',\n",
       " 'img_align_celeba/010238.jpg',\n",
       " 'img_align_celeba/011767.jpg',\n",
       " 'img_align_celeba/010239.jpg',\n",
       " 'img_align_celeba/011773.jpg',\n",
       " 'img_align_celeba/010240.jpg',\n",
       " 'img_align_celeba/011783.jpg',\n",
       " 'img_align_celeba/010241.jpg',\n",
       " 'img_align_celeba/011794.jpg',\n",
       " 'img_align_celeba/010242.jpg',\n",
       " 'img_align_celeba/011796.jpg',\n",
       " 'img_align_celeba/010243.jpg',\n",
       " 'img_align_celeba/011797.jpg',\n",
       " 'img_align_celeba/010244.jpg',\n",
       " 'img_align_celeba/011799.jpg',\n",
       " 'img_align_celeba/010245.jpg',\n",
       " 'img_align_celeba/011810.jpg',\n",
       " 'img_align_celeba/010247.jpg',\n",
       " 'img_align_celeba/011834.jpg',\n",
       " 'img_align_celeba/010248.jpg',\n",
       " 'img_align_celeba/011838.jpg',\n",
       " 'img_align_celeba/010249.jpg',\n",
       " 'img_align_celeba/011840.jpg',\n",
       " 'img_align_celeba/010250.jpg',\n",
       " 'img_align_celeba/011853.jpg',\n",
       " 'img_align_celeba/010251.jpg',\n",
       " 'img_align_celeba/011856.jpg',\n",
       " 'img_align_celeba/010252.jpg',\n",
       " 'img_align_celeba/011862.jpg',\n",
       " 'img_align_celeba/010254.jpg',\n",
       " 'img_align_celeba/011870.jpg',\n",
       " 'img_align_celeba/010255.jpg',\n",
       " 'img_align_celeba/011875.jpg',\n",
       " 'img_align_celeba/010256.jpg',\n",
       " 'img_align_celeba/011879.jpg',\n",
       " 'img_align_celeba/010257.jpg',\n",
       " 'img_align_celeba/011881.jpg',\n",
       " 'img_align_celeba/010258.jpg',\n",
       " 'img_align_celeba/011884.jpg',\n",
       " 'img_align_celeba/010259.jpg',\n",
       " 'img_align_celeba/011891.jpg',\n",
       " 'img_align_celeba/010261.jpg',\n",
       " 'img_align_celeba/011895.jpg',\n",
       " 'img_align_celeba/010263.jpg',\n",
       " 'img_align_celeba/011902.jpg',\n",
       " 'img_align_celeba/010264.jpg',\n",
       " 'img_align_celeba/011906.jpg',\n",
       " 'img_align_celeba/010265.jpg',\n",
       " 'img_align_celeba/011928.jpg',\n",
       " 'img_align_celeba/010267.jpg',\n",
       " 'img_align_celeba/011931.jpg',\n",
       " 'img_align_celeba/010268.jpg',\n",
       " 'img_align_celeba/011946.jpg',\n",
       " 'img_align_celeba/010269.jpg',\n",
       " 'img_align_celeba/011963.jpg',\n",
       " 'img_align_celeba/010270.jpg',\n",
       " 'img_align_celeba/011965.jpg',\n",
       " 'img_align_celeba/010271.jpg',\n",
       " 'img_align_celeba/011968.jpg',\n",
       " 'img_align_celeba/010272.jpg',\n",
       " 'img_align_celeba/011977.jpg',\n",
       " 'img_align_celeba/010273.jpg',\n",
       " 'img_align_celeba/011979.jpg',\n",
       " 'img_align_celeba/010274.jpg',\n",
       " 'img_align_celeba/011984.jpg',\n",
       " 'img_align_celeba/010275.jpg',\n",
       " 'img_align_celeba/011986.jpg',\n",
       " 'img_align_celeba/010276.jpg',\n",
       " 'img_align_celeba/012027.jpg',\n",
       " 'img_align_celeba/010277.jpg',\n",
       " 'img_align_celeba/012047.jpg',\n",
       " 'img_align_celeba/010278.jpg',\n",
       " 'img_align_celeba/012052.jpg',\n",
       " 'img_align_celeba/010280.jpg',\n",
       " 'img_align_celeba/012055.jpg',\n",
       " 'img_align_celeba/010282.jpg',\n",
       " 'img_align_celeba/012072.jpg',\n",
       " 'img_align_celeba/010283.jpg',\n",
       " 'img_align_celeba/012087.jpg',\n",
       " 'img_align_celeba/010284.jpg',\n",
       " 'img_align_celeba/012089.jpg',\n",
       " 'img_align_celeba/010285.jpg',\n",
       " 'img_align_celeba/012090.jpg',\n",
       " 'img_align_celeba/010287.jpg',\n",
       " 'img_align_celeba/012094.jpg',\n",
       " 'img_align_celeba/010288.jpg',\n",
       " 'img_align_celeba/012095.jpg',\n",
       " 'img_align_celeba/010289.jpg',\n",
       " 'img_align_celeba/012124.jpg',\n",
       " 'img_align_celeba/010290.jpg',\n",
       " 'img_align_celeba/012131.jpg',\n",
       " 'img_align_celeba/010291.jpg',\n",
       " 'img_align_celeba/012135.jpg',\n",
       " 'img_align_celeba/010292.jpg',\n",
       " 'img_align_celeba/012177.jpg',\n",
       " 'img_align_celeba/010293.jpg',\n",
       " 'img_align_celeba/012197.jpg',\n",
       " 'img_align_celeba/010294.jpg',\n",
       " 'img_align_celeba/012203.jpg',\n",
       " 'img_align_celeba/010295.jpg',\n",
       " 'img_align_celeba/012209.jpg',\n",
       " 'img_align_celeba/010296.jpg',\n",
       " 'img_align_celeba/012219.jpg',\n",
       " 'img_align_celeba/010297.jpg',\n",
       " 'img_align_celeba/012224.jpg',\n",
       " 'img_align_celeba/010298.jpg',\n",
       " 'img_align_celeba/012228.jpg',\n",
       " 'img_align_celeba/010299.jpg',\n",
       " 'img_align_celeba/012229.jpg',\n",
       " 'img_align_celeba/010300.jpg',\n",
       " 'img_align_celeba/012230.jpg',\n",
       " 'img_align_celeba/010301.jpg',\n",
       " 'img_align_celeba/012232.jpg',\n",
       " 'img_align_celeba/010302.jpg',\n",
       " 'img_align_celeba/012237.jpg',\n",
       " 'img_align_celeba/010303.jpg',\n",
       " 'img_align_celeba/012249.jpg',\n",
       " 'img_align_celeba/010304.jpg',\n",
       " 'img_align_celeba/012258.jpg',\n",
       " 'img_align_celeba/010305.jpg',\n",
       " 'img_align_celeba/012265.jpg',\n",
       " 'img_align_celeba/010306.jpg',\n",
       " 'img_align_celeba/012267.jpg',\n",
       " 'img_align_celeba/010307.jpg',\n",
       " 'img_align_celeba/012275.jpg',\n",
       " 'img_align_celeba/010308.jpg',\n",
       " 'img_align_celeba/012279.jpg',\n",
       " 'img_align_celeba/010310.jpg',\n",
       " 'img_align_celeba/012288.jpg',\n",
       " 'img_align_celeba/010311.jpg',\n",
       " 'img_align_celeba/012289.jpg',\n",
       " 'img_align_celeba/010312.jpg',\n",
       " 'img_align_celeba/012303.jpg',\n",
       " 'img_align_celeba/010313.jpg',\n",
       " 'img_align_celeba/012313.jpg',\n",
       " 'img_align_celeba/010314.jpg',\n",
       " 'img_align_celeba/012316.jpg',\n",
       " 'img_align_celeba/010316.jpg',\n",
       " 'img_align_celeba/012317.jpg',\n",
       " 'img_align_celeba/010317.jpg',\n",
       " 'img_align_celeba/012319.jpg',\n",
       " 'img_align_celeba/010318.jpg',\n",
       " 'img_align_celeba/012332.jpg',\n",
       " 'img_align_celeba/010319.jpg',\n",
       " 'img_align_celeba/012341.jpg',\n",
       " 'img_align_celeba/010320.jpg',\n",
       " 'img_align_celeba/012359.jpg',\n",
       " 'img_align_celeba/010322.jpg',\n",
       " 'img_align_celeba/012371.jpg',\n",
       " 'img_align_celeba/010323.jpg',\n",
       " 'img_align_celeba/012381.jpg',\n",
       " 'img_align_celeba/010324.jpg',\n",
       " 'img_align_celeba/012392.jpg',\n",
       " 'img_align_celeba/010325.jpg',\n",
       " 'img_align_celeba/012394.jpg',\n",
       " 'img_align_celeba/010326.jpg',\n",
       " 'img_align_celeba/012397.jpg',\n",
       " 'img_align_celeba/010327.jpg',\n",
       " 'img_align_celeba/012398.jpg',\n",
       " 'img_align_celeba/010329.jpg',\n",
       " 'img_align_celeba/012399.jpg',\n",
       " 'img_align_celeba/010330.jpg',\n",
       " 'img_align_celeba/012400.jpg',\n",
       " 'img_align_celeba/010331.jpg',\n",
       " 'img_align_celeba/012403.jpg',\n",
       " 'img_align_celeba/010332.jpg',\n",
       " 'img_align_celeba/012405.jpg',\n",
       " 'img_align_celeba/010334.jpg',\n",
       " 'img_align_celeba/012428.jpg',\n",
       " 'img_align_celeba/010335.jpg',\n",
       " 'img_align_celeba/012444.jpg',\n",
       " 'img_align_celeba/010337.jpg',\n",
       " 'img_align_celeba/012475.jpg',\n",
       " 'img_align_celeba/010338.jpg',\n",
       " 'img_align_celeba/012483.jpg',\n",
       " 'img_align_celeba/010339.jpg',\n",
       " 'img_align_celeba/012485.jpg',\n",
       " 'img_align_celeba/010340.jpg',\n",
       " 'img_align_celeba/012492.jpg',\n",
       " 'img_align_celeba/010341.jpg',\n",
       " 'img_align_celeba/012510.jpg',\n",
       " 'img_align_celeba/010342.jpg',\n",
       " 'img_align_celeba/012518.jpg',\n",
       " 'img_align_celeba/010343.jpg',\n",
       " 'img_align_celeba/012519.jpg',\n",
       " 'img_align_celeba/010344.jpg',\n",
       " 'img_align_celeba/012531.jpg',\n",
       " 'img_align_celeba/010345.jpg',\n",
       " 'img_align_celeba/012541.jpg',\n",
       " 'img_align_celeba/010346.jpg',\n",
       " 'img_align_celeba/012547.jpg',\n",
       " 'img_align_celeba/010347.jpg',\n",
       " 'img_align_celeba/012555.jpg',\n",
       " 'img_align_celeba/010348.jpg',\n",
       " 'img_align_celeba/012561.jpg',\n",
       " 'img_align_celeba/010349.jpg',\n",
       " 'img_align_celeba/012577.jpg',\n",
       " 'img_align_celeba/010350.jpg',\n",
       " 'img_align_celeba/012586.jpg',\n",
       " 'img_align_celeba/010351.jpg',\n",
       " 'img_align_celeba/012591.jpg',\n",
       " 'img_align_celeba/010352.jpg',\n",
       " 'img_align_celeba/012598.jpg',\n",
       " 'img_align_celeba/010353.jpg',\n",
       " 'img_align_celeba/012602.jpg',\n",
       " 'img_align_celeba/010354.jpg',\n",
       " 'img_align_celeba/012604.jpg',\n",
       " 'img_align_celeba/010355.jpg',\n",
       " 'img_align_celeba/012611.jpg',\n",
       " 'img_align_celeba/010356.jpg',\n",
       " 'img_align_celeba/012615.jpg',\n",
       " 'img_align_celeba/010357.jpg',\n",
       " 'img_align_celeba/012617.jpg',\n",
       " 'img_align_celeba/010358.jpg',\n",
       " 'img_align_celeba/012623.jpg',\n",
       " 'img_align_celeba/010359.jpg',\n",
       " 'img_align_celeba/012633.jpg',\n",
       " 'img_align_celeba/010360.jpg',\n",
       " 'img_align_celeba/012657.jpg',\n",
       " 'img_align_celeba/010361.jpg',\n",
       " 'img_align_celeba/012679.jpg',\n",
       " 'img_align_celeba/010362.jpg',\n",
       " 'img_align_celeba/012685.jpg',\n",
       " 'img_align_celeba/010363.jpg',\n",
       " 'img_align_celeba/012709.jpg',\n",
       " 'img_align_celeba/010364.jpg',\n",
       " 'img_align_celeba/012712.jpg',\n",
       " 'img_align_celeba/010365.jpg',\n",
       " 'img_align_celeba/012713.jpg',\n",
       " 'img_align_celeba/010366.jpg',\n",
       " 'img_align_celeba/012725.jpg',\n",
       " 'img_align_celeba/010367.jpg',\n",
       " 'img_align_celeba/012747.jpg',\n",
       " 'img_align_celeba/010368.jpg',\n",
       " 'img_align_celeba/012755.jpg',\n",
       " 'img_align_celeba/010369.jpg',\n",
       " 'img_align_celeba/012762.jpg',\n",
       " 'img_align_celeba/010370.jpg',\n",
       " 'img_align_celeba/012765.jpg',\n",
       " 'img_align_celeba/010371.jpg',\n",
       " 'img_align_celeba/012784.jpg',\n",
       " 'img_align_celeba/010372.jpg',\n",
       " 'img_align_celeba/012803.jpg',\n",
       " 'img_align_celeba/010373.jpg',\n",
       " 'img_align_celeba/012807.jpg',\n",
       " 'img_align_celeba/010374.jpg',\n",
       " 'img_align_celeba/012812.jpg',\n",
       " 'img_align_celeba/010376.jpg',\n",
       " 'img_align_celeba/012826.jpg',\n",
       " 'img_align_celeba/010377.jpg',\n",
       " 'img_align_celeba/012840.jpg',\n",
       " 'img_align_celeba/010378.jpg',\n",
       " 'img_align_celeba/012846.jpg',\n",
       " 'img_align_celeba/010379.jpg',\n",
       " 'img_align_celeba/012853.jpg',\n",
       " 'img_align_celeba/010380.jpg',\n",
       " 'img_align_celeba/012866.jpg',\n",
       " 'img_align_celeba/010381.jpg',\n",
       " 'img_align_celeba/012884.jpg',\n",
       " 'img_align_celeba/010382.jpg',\n",
       " 'img_align_celeba/012888.jpg',\n",
       " 'img_align_celeba/010383.jpg',\n",
       " 'img_align_celeba/012890.jpg',\n",
       " 'img_align_celeba/010384.jpg',\n",
       " 'img_align_celeba/012899.jpg',\n",
       " 'img_align_celeba/010385.jpg',\n",
       " 'img_align_celeba/012911.jpg',\n",
       " 'img_align_celeba/010386.jpg',\n",
       " 'img_align_celeba/012916.jpg',\n",
       " 'img_align_celeba/010387.jpg',\n",
       " 'img_align_celeba/012927.jpg',\n",
       " 'img_align_celeba/010388.jpg',\n",
       " 'img_align_celeba/012933.jpg',\n",
       " 'img_align_celeba/010389.jpg',\n",
       " 'img_align_celeba/012938.jpg',\n",
       " 'img_align_celeba/010391.jpg',\n",
       " 'img_align_celeba/012949.jpg',\n",
       " 'img_align_celeba/010392.jpg',\n",
       " 'img_align_celeba/012957.jpg',\n",
       " 'img_align_celeba/010393.jpg',\n",
       " 'img_align_celeba/012966.jpg',\n",
       " 'img_align_celeba/010394.jpg',\n",
       " 'img_align_celeba/012968.jpg',\n",
       " 'img_align_celeba/010395.jpg',\n",
       " 'img_align_celeba/012975.jpg',\n",
       " 'img_align_celeba/010397.jpg',\n",
       " 'img_align_celeba/012976.jpg',\n",
       " 'img_align_celeba/010398.jpg',\n",
       " 'img_align_celeba/012984.jpg',\n",
       " 'img_align_celeba/010399.jpg',\n",
       " 'img_align_celeba/012995.jpg',\n",
       " 'img_align_celeba/010400.jpg',\n",
       " 'img_align_celeba/012998.jpg',\n",
       " 'img_align_celeba/010401.jpg',\n",
       " 'img_align_celeba/012999.jpg',\n",
       " 'img_align_celeba/010402.jpg',\n",
       " 'img_align_celeba/013005.jpg',\n",
       " 'img_align_celeba/010403.jpg',\n",
       " 'img_align_celeba/013007.jpg',\n",
       " 'img_align_celeba/010404.jpg',\n",
       " 'img_align_celeba/013009.jpg',\n",
       " 'img_align_celeba/010405.jpg',\n",
       " 'img_align_celeba/013011.jpg',\n",
       " 'img_align_celeba/010406.jpg',\n",
       " 'img_align_celeba/013014.jpg',\n",
       " 'img_align_celeba/010407.jpg',\n",
       " 'img_align_celeba/013025.jpg',\n",
       " 'img_align_celeba/010408.jpg',\n",
       " 'img_align_celeba/013033.jpg',\n",
       " 'img_align_celeba/010409.jpg',\n",
       " 'img_align_celeba/013039.jpg',\n",
       " 'img_align_celeba/010410.jpg',\n",
       " 'img_align_celeba/013044.jpg',\n",
       " 'img_align_celeba/010412.jpg',\n",
       " 'img_align_celeba/013055.jpg',\n",
       " 'img_align_celeba/010413.jpg',\n",
       " 'img_align_celeba/013058.jpg',\n",
       " 'img_align_celeba/010414.jpg',\n",
       " 'img_align_celeba/013062.jpg',\n",
       " 'img_align_celeba/010415.jpg',\n",
       " 'img_align_celeba/013071.jpg',\n",
       " 'img_align_celeba/010416.jpg',\n",
       " 'img_align_celeba/013072.jpg',\n",
       " 'img_align_celeba/010417.jpg',\n",
       " 'img_align_celeba/013077.jpg',\n",
       " 'img_align_celeba/010419.jpg',\n",
       " 'img_align_celeba/013080.jpg',\n",
       " 'img_align_celeba/010420.jpg',\n",
       " 'img_align_celeba/013091.jpg',\n",
       " 'img_align_celeba/010421.jpg',\n",
       " 'img_align_celeba/013094.jpg',\n",
       " 'img_align_celeba/010422.jpg',\n",
       " 'img_align_celeba/013095.jpg',\n",
       " 'img_align_celeba/010423.jpg',\n",
       " 'img_align_celeba/013096.jpg',\n",
       " 'img_align_celeba/010424.jpg',\n",
       " 'img_align_celeba/013098.jpg',\n",
       " 'img_align_celeba/010425.jpg',\n",
       " 'img_align_celeba/013102.jpg',\n",
       " 'img_align_celeba/010426.jpg',\n",
       " 'img_align_celeba/013106.jpg',\n",
       " 'img_align_celeba/010427.jpg',\n",
       " 'img_align_celeba/013124.jpg',\n",
       " 'img_align_celeba/010428.jpg',\n",
       " 'img_align_celeba/013126.jpg',\n",
       " 'img_align_celeba/010429.jpg',\n",
       " 'img_align_celeba/013130.jpg',\n",
       " 'img_align_celeba/010430.jpg',\n",
       " 'img_align_celeba/013134.jpg',\n",
       " 'img_align_celeba/010431.jpg',\n",
       " 'img_align_celeba/013136.jpg',\n",
       " 'img_align_celeba/010432.jpg',\n",
       " 'img_align_celeba/013137.jpg',\n",
       " 'img_align_celeba/010433.jpg',\n",
       " 'img_align_celeba/013139.jpg',\n",
       " 'img_align_celeba/010434.jpg',\n",
       " 'img_align_celeba/013159.jpg',\n",
       " 'img_align_celeba/010435.jpg',\n",
       " 'img_align_celeba/013164.jpg',\n",
       " 'img_align_celeba/010436.jpg',\n",
       " 'img_align_celeba/013185.jpg',\n",
       " 'img_align_celeba/010439.jpg',\n",
       " 'img_align_celeba/013198.jpg',\n",
       " 'img_align_celeba/010440.jpg',\n",
       " 'img_align_celeba/013204.jpg',\n",
       " 'img_align_celeba/010441.jpg',\n",
       " 'img_align_celeba/013210.jpg',\n",
       " 'img_align_celeba/010442.jpg',\n",
       " 'img_align_celeba/013224.jpg',\n",
       " 'img_align_celeba/010443.jpg',\n",
       " 'img_align_celeba/013227.jpg',\n",
       " 'img_align_celeba/010444.jpg',\n",
       " 'img_align_celeba/013249.jpg',\n",
       " 'img_align_celeba/010445.jpg',\n",
       " 'img_align_celeba/013251.jpg',\n",
       " 'img_align_celeba/010447.jpg',\n",
       " 'img_align_celeba/013252.jpg',\n",
       " 'img_align_celeba/010448.jpg',\n",
       " 'img_align_celeba/013254.jpg',\n",
       " 'img_align_celeba/010450.jpg',\n",
       " 'img_align_celeba/013258.jpg',\n",
       " 'img_align_celeba/010451.jpg',\n",
       " 'img_align_celeba/013267.jpg',\n",
       " 'img_align_celeba/010452.jpg',\n",
       " 'img_align_celeba/013276.jpg',\n",
       " 'img_align_celeba/010453.jpg',\n",
       " 'img_align_celeba/013282.jpg',\n",
       " 'img_align_celeba/010454.jpg',\n",
       " 'img_align_celeba/013285.jpg',\n",
       " 'img_align_celeba/010455.jpg',\n",
       " 'img_align_celeba/013300.jpg',\n",
       " 'img_align_celeba/010456.jpg',\n",
       " 'img_align_celeba/013302.jpg',\n",
       " 'img_align_celeba/010457.jpg',\n",
       " 'img_align_celeba/013305.jpg',\n",
       " 'img_align_celeba/010459.jpg',\n",
       " 'img_align_celeba/013306.jpg',\n",
       " 'img_align_celeba/010460.jpg',\n",
       " 'img_align_celeba/013319.jpg',\n",
       " 'img_align_celeba/010461.jpg',\n",
       " 'img_align_celeba/013329.jpg',\n",
       " 'img_align_celeba/010462.jpg',\n",
       " 'img_align_celeba/013333.jpg',\n",
       " 'img_align_celeba/010463.jpg',\n",
       " 'img_align_celeba/013340.jpg',\n",
       " 'img_align_celeba/010464.jpg',\n",
       " 'img_align_celeba/013341.jpg',\n",
       " 'img_align_celeba/010465.jpg',\n",
       " 'img_align_celeba/013353.jpg',\n",
       " 'img_align_celeba/010466.jpg',\n",
       " 'img_align_celeba/013354.jpg',\n",
       " 'img_align_celeba/010467.jpg',\n",
       " 'img_align_celeba/013394.jpg',\n",
       " 'img_align_celeba/010468.jpg',\n",
       " 'img_align_celeba/013396.jpg',\n",
       " 'img_align_celeba/010469.jpg',\n",
       " 'img_align_celeba/013401.jpg',\n",
       " 'img_align_celeba/010470.jpg',\n",
       " 'img_align_celeba/013404.jpg',\n",
       " 'img_align_celeba/010471.jpg',\n",
       " 'img_align_celeba/013405.jpg',\n",
       " 'img_align_celeba/010472.jpg',\n",
       " 'img_align_celeba/013428.jpg',\n",
       " 'img_align_celeba/010473.jpg',\n",
       " 'img_align_celeba/013451.jpg',\n",
       " 'img_align_celeba/010474.jpg',\n",
       " 'img_align_celeba/013466.jpg',\n",
       " 'img_align_celeba/010475.jpg',\n",
       " 'img_align_celeba/013473.jpg',\n",
       " 'img_align_celeba/010477.jpg',\n",
       " 'img_align_celeba/013474.jpg',\n",
       " 'img_align_celeba/010478.jpg',\n",
       " 'img_align_celeba/013477.jpg',\n",
       " 'img_align_celeba/010479.jpg',\n",
       " 'img_align_celeba/013487.jpg',\n",
       " 'img_align_celeba/010480.jpg',\n",
       " 'img_align_celeba/013494.jpg',\n",
       " 'img_align_celeba/010481.jpg',\n",
       " 'img_align_celeba/013498.jpg',\n",
       " 'img_align_celeba/010482.jpg',\n",
       " 'img_align_celeba/013500.jpg',\n",
       " 'img_align_celeba/010483.jpg',\n",
       " 'img_align_celeba/013503.jpg',\n",
       " 'img_align_celeba/010484.jpg',\n",
       " 'img_align_celeba/013506.jpg',\n",
       " 'img_align_celeba/010486.jpg',\n",
       " 'img_align_celeba/013533.jpg',\n",
       " 'img_align_celeba/010487.jpg',\n",
       " 'img_align_celeba/013537.jpg',\n",
       " 'img_align_celeba/010488.jpg',\n",
       " 'img_align_celeba/013546.jpg',\n",
       " 'img_align_celeba/010489.jpg',\n",
       " 'img_align_celeba/013553.jpg',\n",
       " 'img_align_celeba/010492.jpg',\n",
       " 'img_align_celeba/013568.jpg',\n",
       " 'img_align_celeba/010494.jpg',\n",
       " 'img_align_celeba/013569.jpg',\n",
       " 'img_align_celeba/010495.jpg',\n",
       " 'img_align_celeba/013579.jpg',\n",
       " 'img_align_celeba/010496.jpg',\n",
       " 'img_align_celeba/013582.jpg',\n",
       " 'img_align_celeba/010497.jpg',\n",
       " 'img_align_celeba/013595.jpg',\n",
       " 'img_align_celeba/010498.jpg',\n",
       " 'img_align_celeba/013606.jpg',\n",
       " 'img_align_celeba/010499.jpg',\n",
       " 'img_align_celeba/013608.jpg',\n",
       " 'img_align_celeba/010500.jpg',\n",
       " 'img_align_celeba/013611.jpg',\n",
       " 'img_align_celeba/010501.jpg',\n",
       " 'img_align_celeba/013627.jpg',\n",
       " 'img_align_celeba/010502.jpg',\n",
       " 'img_align_celeba/013629.jpg',\n",
       " 'img_align_celeba/010503.jpg',\n",
       " 'img_align_celeba/013631.jpg',\n",
       " 'img_align_celeba/010504.jpg',\n",
       " 'img_align_celeba/013633.jpg',\n",
       " 'img_align_celeba/010507.jpg',\n",
       " 'img_align_celeba/013643.jpg',\n",
       " 'img_align_celeba/010509.jpg',\n",
       " 'img_align_celeba/013646.jpg',\n",
       " 'img_align_celeba/010511.jpg',\n",
       " 'img_align_celeba/013649.jpg',\n",
       " 'img_align_celeba/010512.jpg',\n",
       " 'img_align_celeba/013683.jpg',\n",
       " 'img_align_celeba/010513.jpg',\n",
       " 'img_align_celeba/013697.jpg',\n",
       " 'img_align_celeba/010514.jpg',\n",
       " 'img_align_celeba/013700.jpg',\n",
       " 'img_align_celeba/010515.jpg',\n",
       " 'img_align_celeba/013706.jpg',\n",
       " 'img_align_celeba/010516.jpg',\n",
       " 'img_align_celeba/013709.jpg',\n",
       " 'img_align_celeba/010517.jpg',\n",
       " 'img_align_celeba/013710.jpg',\n",
       " 'img_align_celeba/010519.jpg',\n",
       " 'img_align_celeba/013711.jpg',\n",
       " 'img_align_celeba/010520.jpg',\n",
       " 'img_align_celeba/013760.jpg',\n",
       " 'img_align_celeba/010521.jpg',\n",
       " 'img_align_celeba/013771.jpg',\n",
       " 'img_align_celeba/010522.jpg',\n",
       " 'img_align_celeba/013773.jpg',\n",
       " 'img_align_celeba/010523.jpg',\n",
       " 'img_align_celeba/013780.jpg',\n",
       " 'img_align_celeba/010524.jpg',\n",
       " 'img_align_celeba/013787.jpg',\n",
       " 'img_align_celeba/010525.jpg',\n",
       " 'img_align_celeba/013796.jpg',\n",
       " 'img_align_celeba/010526.jpg',\n",
       " 'img_align_celeba/013808.jpg',\n",
       " 'img_align_celeba/010527.jpg',\n",
       " 'img_align_celeba/013824.jpg',\n",
       " 'img_align_celeba/010528.jpg',\n",
       " 'img_align_celeba/013825.jpg',\n",
       " 'img_align_celeba/010529.jpg',\n",
       " 'img_align_celeba/013849.jpg',\n",
       " 'img_align_celeba/010530.jpg',\n",
       " 'img_align_celeba/013851.jpg',\n",
       " 'img_align_celeba/010531.jpg',\n",
       " 'img_align_celeba/013853.jpg',\n",
       " 'img_align_celeba/010532.jpg',\n",
       " 'img_align_celeba/013862.jpg',\n",
       " 'img_align_celeba/010533.jpg',\n",
       " 'img_align_celeba/013874.jpg',\n",
       " 'img_align_celeba/010534.jpg',\n",
       " 'img_align_celeba/013879.jpg',\n",
       " 'img_align_celeba/010535.jpg',\n",
       " 'img_align_celeba/013881.jpg',\n",
       " 'img_align_celeba/010536.jpg',\n",
       " 'img_align_celeba/013883.jpg',\n",
       " 'img_align_celeba/010538.jpg',\n",
       " 'img_align_celeba/013892.jpg',\n",
       " 'img_align_celeba/010539.jpg',\n",
       " 'img_align_celeba/013897.jpg',\n",
       " 'img_align_celeba/010540.jpg',\n",
       " 'img_align_celeba/013899.jpg',\n",
       " 'img_align_celeba/010541.jpg',\n",
       " 'img_align_celeba/013901.jpg',\n",
       " 'img_align_celeba/010542.jpg',\n",
       " 'img_align_celeba/013908.jpg',\n",
       " 'img_align_celeba/010543.jpg',\n",
       " 'img_align_celeba/013930.jpg',\n",
       " 'img_align_celeba/010544.jpg',\n",
       " 'img_align_celeba/013944.jpg',\n",
       " 'img_align_celeba/010545.jpg',\n",
       " 'img_align_celeba/013954.jpg',\n",
       " 'img_align_celeba/010546.jpg',\n",
       " 'img_align_celeba/013956.jpg',\n",
       " 'img_align_celeba/010547.jpg',\n",
       " 'img_align_celeba/013961.jpg',\n",
       " 'img_align_celeba/010548.jpg',\n",
       " 'img_align_celeba/013982.jpg',\n",
       " 'img_align_celeba/010551.jpg',\n",
       " 'img_align_celeba/014007.jpg',\n",
       " 'img_align_celeba/010552.jpg',\n",
       " 'img_align_celeba/014018.jpg',\n",
       " 'img_align_celeba/010553.jpg',\n",
       " 'img_align_celeba/014045.jpg',\n",
       " 'img_align_celeba/010554.jpg',\n",
       " 'img_align_celeba/014057.jpg',\n",
       " 'img_align_celeba/010555.jpg',\n",
       " 'img_align_celeba/014063.jpg',\n",
       " 'img_align_celeba/010556.jpg',\n",
       " 'img_align_celeba/014086.jpg',\n",
       " 'img_align_celeba/010557.jpg',\n",
       " 'img_align_celeba/014099.jpg',\n",
       " 'img_align_celeba/010558.jpg',\n",
       " 'img_align_celeba/014109.jpg',\n",
       " 'img_align_celeba/010559.jpg',\n",
       " 'img_align_celeba/014115.jpg',\n",
       " 'img_align_celeba/010560.jpg',\n",
       " 'img_align_celeba/014130.jpg',\n",
       " 'img_align_celeba/010561.jpg',\n",
       " 'img_align_celeba/014139.jpg',\n",
       " 'img_align_celeba/010562.jpg',\n",
       " 'img_align_celeba/014164.jpg',\n",
       " 'img_align_celeba/010563.jpg',\n",
       " 'img_align_celeba/014177.jpg',\n",
       " 'img_align_celeba/010564.jpg',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "id": "TLowhfkuDAkF",
    "outputId": "b77bd16d-2c3a-4854-fcc3-c9fd043a4be9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "p9dPMjLZDKRs",
    "outputId": "46729c5a-eca5-44ec-84b8-275b50993351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 50)        650       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 50)        10050     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 50)        10050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 50)        10050     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 50)        10050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 50)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               1600500   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 1,642,352\n",
      "Trainable params: 1,642,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "\n",
    "# 모델 구성하기\n",
    "model = Sequential()# Sequential은 계층을 선형으로 쌓은 것입니다.\n",
    "\n",
    "model.add(Conv2D(filters=50, kernel_size=2, padding=\"same\", input_shape=(64, 64, 3), activation=\"relu\"))\n",
    "# 첫 번째 숫자인 32는 32개의 필터를 적용하겠다는 의미입니다. kernel_size는 필터의 크기를 의미합니다.\n",
    "# input_shape는 (행, 열, 색상)을 의미합니다. 흑백의 경우 1의 값을 가집니다. RGB의 경우 3의 값을 가집니다.\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "#맥스 풀링은 정해진 영역 안에서 가장 큰 값만 남기고 나머지는 버리는 방식입니다. \n",
    "#pool_size는 풀링 윈도우 크기이며 2의 값은 전체 크기를 절반으로 줄입니다.\n",
    "\n",
    "model.add(Conv2D(filters=50, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=50, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=50, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=50, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "# Dropout()는 특정 노드에 학습이 지나치게 몰리는 것을 방지하기 위해 랜덤하게 일부 노드를 꺼주는 역할을 합니다. \n",
    "#Dropout을 통해 과적합을 조금 더 효과적으로 회피할 수 있습니다.\n",
    "\n",
    "model.add(Flatten())\n",
    "# 지금까지 작업했던 이미지는 2차원 배열인데, Flattern() 함수를 통해 1차원 배열로 바꿔줄 수 있습니다.\n",
    "\n",
    "model.add(Dense(500, activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "#Dense 레이어는 입력과 출력을 모두 연결해준다. 입력 뉴런과 출력 뉴런을 모두 연결한다고 해서 전결합층이라고 불린다. \n",
    "#첫번째 인자는 출력 뉴런의 수를 설정한다. activation은 활성화 함수를 설정하는 부분이다.\n",
    "\n",
    "model.summary()\n",
    "#summary()함수로 자신이 생성한 모델의 레이어, 출력 모양, 파라미터 개수 등을 체크할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "# 모델 학습과정 설정하기\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MWxUidLzj9sg",
    "outputId": "1c99c3f8-b8cf-4b9a-dc70-016f893a36f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H59xU5MTkFZx"
   },
   "outputs": [],
   "source": [
    "#학습시키기 전 이미지 사이즈를 조정하고 images, labels에 이미지와 카테고리를 np.asarray로 배열화하여 반환한다. 여기서는 학습시간을 단축시키기 위해 이미지 사이즈를 (64, 64)로 주었다. \n",
    "#이부분을 자유롭게 조정할 수 있으며, 앞에서 input shape으로 주었던 부분과 사이즈를 맞춰주어야 한다. 이미지 사이즈를 크게 줄 경우 계산량이 증가하여 학습이 지나치게 길어질 수 있다.\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.utils import np_utils\n",
    "import imageio\n",
    "\n",
    "def prepare_train(file_paths, labels):\n",
    "  images = [imageio.imread(path) for path in file_paths]\n",
    "  new_img=[]\n",
    "  for i in range(len(images)):\n",
    "    img=cv2.resize(images[i],(64,64))\n",
    "    new_img.append(img)\n",
    "    \n",
    "     \n",
    "  images=np.asarray(new_img)\n",
    "  images=images/255\n",
    "  \n",
    "  labels=np.asarray(labels)\n",
    "  labels=np_utils.to_categorical(labels,2)\n",
    "  \n",
    "  return images,labels\n",
    "#(오류가 발생할 경우!)\n",
    "#imageio.imread(path)이 코드는 본인 사용환경의 SciPy의 버전에 따라 imageio 혹은 imread로 쓸 수 있다.\n",
    "#imread는 SciPy 1.0.0에서 사용되지 않으며 1.2.0에서 제거되었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51697
    },
    "colab_type": "code",
    "id": "nnjJ8XV8lAGD",
    "outputId": "8defd462-c514-4cee-b34b-348331c11d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute:  0\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.2495 - acc: 0.8957 - val_loss: 0.3834 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38336, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.2194 - acc: 0.9055 - val_loss: 0.3757 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38336 to 0.37566, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.2055 - acc: 0.9169 - val_loss: 0.4701 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37566\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.1866 - acc: 0.9235 - val_loss: 0.5376 - val_acc: 0.7860\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37566\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.1609 - acc: 0.9335 - val_loss: 0.5127 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37566\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.1407 - acc: 0.9447 - val_loss: 0.4931 - val_acc: 0.8175\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37566\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.1145 - acc: 0.9567 - val_loss: 0.5176 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37566\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.0979 - acc: 0.9619 - val_loss: 0.5649 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37566\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.0949 - acc: 0.9648 - val_loss: 0.5807 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37566\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.0802 - acc: 0.9683 - val_loss: 0.8138 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37566\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.0745 - acc: 0.9725 - val_loss: 0.6656 - val_acc: 0.8035\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37566\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.0598 - acc: 0.9794 - val_loss: 0.6543 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.37566\n",
      "Epoch 00012: early stopping\n",
      "Attribute:  1\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.8524 - acc: 0.5479 - val_loss: 0.6665 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66646, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6307 - acc: 0.6509 - val_loss: 0.6546 - val_acc: 0.6115\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66646 to 0.65464, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.6057 - acc: 0.6805 - val_loss: 0.6044 - val_acc: 0.6810\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65464 to 0.60437, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.5858 - acc: 0.7004 - val_loss: 0.5953 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60437 to 0.59526, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.5844 - acc: 0.6917 - val_loss: 0.5861 - val_acc: 0.6950\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.59526 to 0.58607, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.5663 - acc: 0.7116 - val_loss: 0.5792 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.58607 to 0.57922, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.5578 - acc: 0.7183 - val_loss: 0.5738 - val_acc: 0.7045\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.57922 to 0.57377, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.5453 - acc: 0.7310 - val_loss: 0.5689 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.57377 to 0.56892, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.5408 - acc: 0.7336 - val_loss: 0.5719 - val_acc: 0.7115\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.56892\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.5267 - acc: 0.7468 - val_loss: 0.5605 - val_acc: 0.7085\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.56892 to 0.56051, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.5135 - acc: 0.7524 - val_loss: 0.5539 - val_acc: 0.7195\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.56051 to 0.55385, saving model to model.weights.best.hdf5\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.5042 - acc: 0.7532 - val_loss: 0.5574 - val_acc: 0.7175\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.55385\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.4905 - acc: 0.7659 - val_loss: 0.5477 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.55385 to 0.54765, saving model to model.weights.best.hdf5\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.4830 - acc: 0.7660 - val_loss: 0.5415 - val_acc: 0.7315\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.54765 to 0.54149, saving model to model.weights.best.hdf5\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.4682 - acc: 0.7775 - val_loss: 0.5421 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.54149\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.4561 - acc: 0.7884 - val_loss: 0.5386 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.54149 to 0.53856, saving model to model.weights.best.hdf5\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.4407 - acc: 0.7955 - val_loss: 0.5390 - val_acc: 0.7185\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.53856\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.4214 - acc: 0.8052 - val_loss: 0.5427 - val_acc: 0.7230\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.53856\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.4092 - acc: 0.8074 - val_loss: 0.5535 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.53856\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.3891 - acc: 0.8187 - val_loss: 0.5524 - val_acc: 0.7245\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.53856\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.3635 - acc: 0.8384 - val_loss: 0.5702 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.53856\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.3434 - acc: 0.8483 - val_loss: 0.6241 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.53856\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.3267 - acc: 0.8534 - val_loss: 0.6009 - val_acc: 0.7190\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.53856\n",
      "Epoch 24/50\n",
      " - 4s - loss: 0.2921 - acc: 0.8760 - val_loss: 0.6377 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.53856\n",
      "Epoch 25/50\n",
      " - 4s - loss: 0.2765 - acc: 0.8816 - val_loss: 0.6772 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.53856\n",
      "Epoch 26/50\n",
      " - 4s - loss: 0.2588 - acc: 0.8874 - val_loss: 0.6591 - val_acc: 0.7205\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.53856\n",
      "Epoch 00026: early stopping\n",
      "Attribute:  2\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6073 - acc: 0.6836 - val_loss: 0.5548 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55480, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.5372 - acc: 0.7359 - val_loss: 0.5134 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55480 to 0.51337, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5184 - acc: 0.7449 - val_loss: 0.4958 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51337 to 0.49580, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 5s - loss: 0.4915 - acc: 0.7590 - val_loss: 0.5523 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49580\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.4802 - acc: 0.7664 - val_loss: 0.5176 - val_acc: 0.7495\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49580\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.4607 - acc: 0.7827 - val_loss: 0.5212 - val_acc: 0.7485\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49580\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.4495 - acc: 0.7828 - val_loss: 0.4969 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49580\n",
      "Epoch 8/50\n",
      " - 5s - loss: 0.4310 - acc: 0.7946 - val_loss: 0.4936 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49580 to 0.49359, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.4100 - acc: 0.8080 - val_loss: 0.5114 - val_acc: 0.7635\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49359\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.3976 - acc: 0.8184 - val_loss: 0.5035 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49359\n",
      "Epoch 11/50\n",
      " - 5s - loss: 0.3779 - acc: 0.8241 - val_loss: 0.5164 - val_acc: 0.7585\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49359\n",
      "Epoch 12/50\n",
      " - 5s - loss: 0.3532 - acc: 0.8386 - val_loss: 0.5351 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49359\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.3336 - acc: 0.8565 - val_loss: 0.5615 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49359\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.3105 - acc: 0.8648 - val_loss: 0.5600 - val_acc: 0.7585\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49359\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.2787 - acc: 0.8830 - val_loss: 0.6045 - val_acc: 0.7340\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49359\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.2496 - acc: 0.8916 - val_loss: 0.6288 - val_acc: 0.7485\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.49359\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.2219 - acc: 0.9093 - val_loss: 0.6666 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49359\n",
      "Epoch 18/50\n",
      " - 5s - loss: 0.1880 - acc: 0.9250 - val_loss: 0.6977 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49359\n",
      "Epoch 00018: early stopping\n",
      "Attribute:  3\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7254 - acc: 0.5478 - val_loss: 0.7183 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71829, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6509 - acc: 0.6246 - val_loss: 0.6399 - val_acc: 0.6710\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71829 to 0.63990, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 5s - loss: 0.6248 - acc: 0.6531 - val_loss: 0.6578 - val_acc: 0.6155\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63990\n",
      "Epoch 4/50\n",
      " - 5s - loss: 0.6116 - acc: 0.6699 - val_loss: 0.7429 - val_acc: 0.5375\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63990\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.5922 - acc: 0.6907 - val_loss: 0.6084 - val_acc: 0.6590\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63990 to 0.60836, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.5704 - acc: 0.7036 - val_loss: 0.6986 - val_acc: 0.5715\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.60836\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.5561 - acc: 0.7140 - val_loss: 0.5484 - val_acc: 0.6905\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.60836 to 0.54844, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.5333 - acc: 0.7272 - val_loss: 0.4520 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.54844 to 0.45200, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.5130 - acc: 0.7449 - val_loss: 0.6425 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.45200\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.4748 - acc: 0.7695 - val_loss: 0.5735 - val_acc: 0.6995\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.45200\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.4403 - acc: 0.7893 - val_loss: 0.6700 - val_acc: 0.6465\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.45200\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.3963 - acc: 0.8199 - val_loss: 0.5890 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.45200\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.3603 - acc: 0.8404 - val_loss: 0.6510 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.45200\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.3278 - acc: 0.8544 - val_loss: 0.7870 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.45200\n",
      "Epoch 15/50\n",
      " - 5s - loss: 0.2780 - acc: 0.8847 - val_loss: 0.9229 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.45200\n",
      "Epoch 16/50\n",
      " - 5s - loss: 0.2455 - acc: 0.9005 - val_loss: 0.5717 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.45200\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.2054 - acc: 0.9131 - val_loss: 0.9119 - val_acc: 0.6795\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.45200\n",
      "Epoch 18/50\n",
      " - 5s - loss: 0.1778 - acc: 0.9276 - val_loss: 0.9925 - val_acc: 0.6545\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.45200\n",
      "Epoch 00018: early stopping\n",
      "Attribute:  4\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.4148 - acc: 0.8124 - val_loss: 0.3612 - val_acc: 0.8395\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36118, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.2587 - acc: 0.9021 - val_loss: 0.1844 - val_acc: 0.9205\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36118 to 0.18444, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.2083 - acc: 0.9199 - val_loss: 0.2363 - val_acc: 0.8925\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.18444\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.1743 - acc: 0.9339 - val_loss: 0.2005 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.18444\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.1426 - acc: 0.9482 - val_loss: 0.1663 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.18444 to 0.16631, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.1234 - acc: 0.9555 - val_loss: 0.2818 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16631\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.1137 - acc: 0.9580 - val_loss: 0.2048 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.16631\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.0895 - acc: 0.9681 - val_loss: 0.2262 - val_acc: 0.9260\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16631\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.0750 - acc: 0.9735 - val_loss: 0.1821 - val_acc: 0.9445\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16631\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.0654 - acc: 0.9770 - val_loss: 0.2548 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.16631\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.0520 - acc: 0.9813 - val_loss: 0.2545 - val_acc: 0.9260\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.16631\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.0536 - acc: 0.9804 - val_loss: 0.1724 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.16631\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.0453 - acc: 0.9848 - val_loss: 0.2280 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.16631\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.0294 - acc: 0.9910 - val_loss: 0.2097 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.16631\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0297 - acc: 0.9906 - val_loss: 0.2135 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.16631\n",
      "Epoch 00015: early stopping\n",
      "Attribute:  5\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7023 - acc: 0.6694 - val_loss: 0.4415 - val_acc: 0.8010\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44152, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4060 - acc: 0.8060 - val_loss: 0.3878 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44152 to 0.38781, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.3585 - acc: 0.8381 - val_loss: 0.2957 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38781 to 0.29573, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3183 - acc: 0.8666 - val_loss: 0.2935 - val_acc: 0.8770\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.29573 to 0.29347, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2794 - acc: 0.8884 - val_loss: 0.2856 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.29347 to 0.28564, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2444 - acc: 0.8995 - val_loss: 0.2449 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.28564 to 0.24489, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2507 - acc: 0.8969 - val_loss: 0.2506 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24489\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2256 - acc: 0.9100 - val_loss: 0.2362 - val_acc: 0.9070\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24489 to 0.23616, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2197 - acc: 0.9125 - val_loss: 0.2496 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.23616\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1998 - acc: 0.9202 - val_loss: 0.2291 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.23616 to 0.22906, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1815 - acc: 0.9284 - val_loss: 0.2500 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22906\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1909 - acc: 0.9232 - val_loss: 0.2368 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22906\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1665 - acc: 0.9373 - val_loss: 0.2562 - val_acc: 0.9070\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22906\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1535 - acc: 0.9394 - val_loss: 0.2554 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22906\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1477 - acc: 0.9435 - val_loss: 0.2534 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.22906\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.1352 - acc: 0.9491 - val_loss: 0.2378 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.22906\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.1284 - acc: 0.9509 - val_loss: 0.2591 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.22906\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.1290 - acc: 0.9487 - val_loss: 0.2787 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.22906\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.0987 - acc: 0.9605 - val_loss: 0.2375 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.22906\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.0957 - acc: 0.9619 - val_loss: 0.2472 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22906\n",
      "Epoch 00020: early stopping\n",
      "Attribute:  6\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7530 - acc: 0.5242 - val_loss: 0.7057 - val_acc: 0.4660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70570, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6858 - acc: 0.5545 - val_loss: 0.6871 - val_acc: 0.5390\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70570 to 0.68706, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.6803 - acc: 0.5649 - val_loss: 0.6771 - val_acc: 0.5680\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68706 to 0.67711, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.6789 - acc: 0.5649 - val_loss: 0.7014 - val_acc: 0.5070\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.67711\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.6774 - acc: 0.5689 - val_loss: 0.6751 - val_acc: 0.5520\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.67711 to 0.67509, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.6770 - acc: 0.5705 - val_loss: 0.6839 - val_acc: 0.5515\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.67509\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.6719 - acc: 0.5763 - val_loss: 0.6908 - val_acc: 0.5290\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.67509\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.6677 - acc: 0.5889 - val_loss: 0.6873 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.67509\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.6614 - acc: 0.5984 - val_loss: 0.6712 - val_acc: 0.5725\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.67509 to 0.67116, saving model to model.weights.best.hdf5\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.6562 - acc: 0.6001 - val_loss: 0.6881 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.67116\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.6465 - acc: 0.6127 - val_loss: 0.6927 - val_acc: 0.5235\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.67116\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.6355 - acc: 0.6200 - val_loss: 0.7000 - val_acc: 0.5280\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.67116\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.6258 - acc: 0.6308 - val_loss: 0.7035 - val_acc: 0.5490\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.67116\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.6160 - acc: 0.6417 - val_loss: 0.6945 - val_acc: 0.5770\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.67116\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.6026 - acc: 0.6520 - val_loss: 0.7076 - val_acc: 0.5685\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.67116\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.5849 - acc: 0.6689 - val_loss: 0.7135 - val_acc: 0.5595\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.67116\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.5649 - acc: 0.6950 - val_loss: 0.7216 - val_acc: 0.5630\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.67116\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.5425 - acc: 0.7017 - val_loss: 0.7407 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.67116\n",
      "Epoch 19/50\n",
      " - 5s - loss: 0.5177 - acc: 0.7221 - val_loss: 0.7447 - val_acc: 0.5595\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.67116\n",
      "Epoch 00019: early stopping\n",
      "Attribute:  7\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6926 - acc: 0.5416 - val_loss: 0.6438 - val_acc: 0.6435\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64375, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 5s - loss: 0.6612 - acc: 0.6176 - val_loss: 0.6940 - val_acc: 0.5535\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.64375\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.6530 - acc: 0.6269 - val_loss: 0.6365 - val_acc: 0.6485\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64375 to 0.63645, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.6440 - acc: 0.6381 - val_loss: 0.6406 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63645\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.6413 - acc: 0.6390 - val_loss: 0.6419 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63645\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.6315 - acc: 0.6509 - val_loss: 0.6099 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.63645 to 0.60990, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.6199 - acc: 0.6653 - val_loss: 0.6359 - val_acc: 0.6510\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.60990\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.6041 - acc: 0.6815 - val_loss: 0.6011 - val_acc: 0.6770\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.60990 to 0.60110, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.5975 - acc: 0.6745 - val_loss: 0.6344 - val_acc: 0.6585\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.60110\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.5874 - acc: 0.6854 - val_loss: 0.5910 - val_acc: 0.6770\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.60110 to 0.59097, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.5700 - acc: 0.7036 - val_loss: 0.6157 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.59097\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.5547 - acc: 0.7090 - val_loss: 0.6065 - val_acc: 0.6630\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.59097\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.5348 - acc: 0.7296 - val_loss: 0.5882 - val_acc: 0.6975\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.59097 to 0.58819, saving model to model.weights.best.hdf5\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.5304 - acc: 0.7284 - val_loss: 0.5868 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.58819 to 0.58682, saving model to model.weights.best.hdf5\n",
      "Epoch 15/50\n",
      " - 5s - loss: 0.5010 - acc: 0.7556 - val_loss: 0.6072 - val_acc: 0.6865\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.58682\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.4759 - acc: 0.7706 - val_loss: 0.6244 - val_acc: 0.6910\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.58682\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.4385 - acc: 0.7892 - val_loss: 0.6449 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.58682\n",
      "Epoch 18/50\n",
      " - 5s - loss: 0.4117 - acc: 0.8091 - val_loss: 0.7336 - val_acc: 0.6505\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.58682\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.3776 - acc: 0.8266 - val_loss: 0.7265 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.58682\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.3297 - acc: 0.8500 - val_loss: 0.7949 - val_acc: 0.6575\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.58682\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.3033 - acc: 0.8675 - val_loss: 0.9141 - val_acc: 0.5975\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.58682\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.2851 - acc: 0.8780 - val_loss: 0.8107 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.58682\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.2463 - acc: 0.8957 - val_loss: 0.8699 - val_acc: 0.6615\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.58682\n",
      "Epoch 24/50\n",
      " - 5s - loss: 0.2068 - acc: 0.9160 - val_loss: 1.0366 - val_acc: 0.6325\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.58682\n",
      "Epoch 00024: early stopping\n",
      "Attribute:  8\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.5341 - acc: 0.7407 - val_loss: 0.4352 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43524, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4122 - acc: 0.8154 - val_loss: 0.4093 - val_acc: 0.8115\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43524 to 0.40931, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.3666 - acc: 0.8349 - val_loss: 0.5378 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.40931\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3609 - acc: 0.8391 - val_loss: 0.3977 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.40931 to 0.39774, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.3243 - acc: 0.8563 - val_loss: 0.3939 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.39774 to 0.39389, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.3105 - acc: 0.8635 - val_loss: 0.3721 - val_acc: 0.8330\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.39389 to 0.37210, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.3064 - acc: 0.8693 - val_loss: 0.3787 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37210\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2639 - acc: 0.8874 - val_loss: 0.3879 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37210\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2484 - acc: 0.8976 - val_loss: 0.4000 - val_acc: 0.8305\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37210\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.2222 - acc: 0.9079 - val_loss: 0.4344 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37210\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1997 - acc: 0.9195 - val_loss: 0.5502 - val_acc: 0.7985\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37210\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1823 - acc: 0.9255 - val_loss: 0.4726 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.37210\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1457 - acc: 0.9426 - val_loss: 0.4865 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.37210\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1432 - acc: 0.9447 - val_loss: 0.5052 - val_acc: 0.8175\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.37210\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1074 - acc: 0.9623 - val_loss: 0.6078 - val_acc: 0.8190\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.37210\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0918 - acc: 0.9676 - val_loss: 0.5668 - val_acc: 0.8285\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.37210\n",
      "Epoch 00016: early stopping\n",
      "Attribute:  9\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6806 - acc: 0.6663 - val_loss: 0.2914 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29141, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.3107 - acc: 0.8682 - val_loss: 0.2967 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.29141\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.2538 - acc: 0.8960 - val_loss: 0.2096 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.29141 to 0.20956, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.2403 - acc: 0.9003 - val_loss: 0.1937 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.20956 to 0.19366, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2235 - acc: 0.9115 - val_loss: 0.2700 - val_acc: 0.8870\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.19366\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2169 - acc: 0.9113 - val_loss: 0.2903 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.19366\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.1933 - acc: 0.9245 - val_loss: 0.2206 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.19366\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.1906 - acc: 0.9223 - val_loss: 0.2041 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.19366\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.1893 - acc: 0.9274 - val_loss: 0.1930 - val_acc: 0.9215\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.19366 to 0.19297, saving model to model.weights.best.hdf5\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1652 - acc: 0.9351 - val_loss: 0.1983 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.19297\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1626 - acc: 0.9365 - val_loss: 0.1840 - val_acc: 0.9275\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.19297 to 0.18404, saving model to model.weights.best.hdf5\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1419 - acc: 0.9420 - val_loss: 0.2115 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.18404\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1288 - acc: 0.9506 - val_loss: 0.1821 - val_acc: 0.9285\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.18404 to 0.18206, saving model to model.weights.best.hdf5\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1173 - acc: 0.9546 - val_loss: 0.1926 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18206\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1043 - acc: 0.9624 - val_loss: 0.2271 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.18206\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0886 - acc: 0.9685 - val_loss: 0.2061 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18206\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0770 - acc: 0.9736 - val_loss: 0.2048 - val_acc: 0.9345\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18206\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.0704 - acc: 0.9743 - val_loss: 0.2087 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18206\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.0626 - acc: 0.9769 - val_loss: 0.2553 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18206\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.0543 - acc: 0.9814 - val_loss: 0.3111 - val_acc: 0.9065\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18206\n",
      "Epoch 21/50\n",
      " - 5s - loss: 0.0495 - acc: 0.9810 - val_loss: 0.2807 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.18206\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.0394 - acc: 0.9863 - val_loss: 0.2775 - val_acc: 0.9285\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.18206\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.0369 - acc: 0.9875 - val_loss: 0.2846 - val_acc: 0.9205\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.18206\n",
      "Epoch 00023: early stopping\n",
      "Attribute:  10\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7557 - acc: 0.5800 - val_loss: 0.5235 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52347, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 5s - loss: 0.5908 - acc: 0.6820 - val_loss: 0.6196 - val_acc: 0.6540\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.52347\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5180 - acc: 0.7474 - val_loss: 0.5715 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52347\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.4759 - acc: 0.7748 - val_loss: 0.4417 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52347 to 0.44171, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.4500 - acc: 0.7870 - val_loss: 0.4326 - val_acc: 0.7985\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.44171 to 0.43263, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.4309 - acc: 0.8024 - val_loss: 0.4839 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43263\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.4115 - acc: 0.8118 - val_loss: 0.4110 - val_acc: 0.8075\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.43263 to 0.41099, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.3852 - acc: 0.8265 - val_loss: 0.4632 - val_acc: 0.7695\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.41099\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.3626 - acc: 0.8388 - val_loss: 0.3566 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.41099 to 0.35661, saving model to model.weights.best.hdf5\n",
      "Epoch 10/50\n",
      " - 5s - loss: 0.3392 - acc: 0.8525 - val_loss: 0.3932 - val_acc: 0.8285\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35661\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.3184 - acc: 0.8630 - val_loss: 0.3655 - val_acc: 0.8420\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35661\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2961 - acc: 0.8711 - val_loss: 0.4006 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35661\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.2706 - acc: 0.8831 - val_loss: 0.4502 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35661\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.2442 - acc: 0.8930 - val_loss: 0.3392 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.35661 to 0.33920, saving model to model.weights.best.hdf5\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.2235 - acc: 0.9084 - val_loss: 0.3444 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.33920\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.2067 - acc: 0.9149 - val_loss: 0.6213 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.33920\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.1743 - acc: 0.9300 - val_loss: 0.3760 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.33920\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.1432 - acc: 0.9412 - val_loss: 0.3947 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.33920\n",
      "Epoch 19/50\n",
      " - 5s - loss: 0.1151 - acc: 0.9575 - val_loss: 0.4540 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.33920\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.1150 - acc: 0.9564 - val_loss: 0.4346 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.33920\n",
      "Epoch 21/50\n",
      " - 5s - loss: 0.1008 - acc: 0.9649 - val_loss: 0.6896 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.33920\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.0802 - acc: 0.9720 - val_loss: 0.7160 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.33920\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.0601 - acc: 0.9811 - val_loss: 0.5954 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.33920\n",
      "Epoch 24/50\n",
      " - 4s - loss: 0.0615 - acc: 0.9794 - val_loss: 0.7895 - val_acc: 0.7745\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.33920\n",
      "Epoch 00024: early stopping\n",
      "Attribute:  11\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.7219 - acc: 0.5980 - val_loss: 0.6673 - val_acc: 0.5685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66725, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.5342 - acc: 0.7399 - val_loss: 0.5341 - val_acc: 0.7185\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66725 to 0.53405, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.4885 - acc: 0.7706 - val_loss: 0.4032 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53405 to 0.40319, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.4540 - acc: 0.7946 - val_loss: 0.4100 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.40319\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.4429 - acc: 0.7992 - val_loss: 0.4521 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.40319\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.4228 - acc: 0.8110 - val_loss: 0.5073 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.40319\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.4271 - acc: 0.8040 - val_loss: 0.4190 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.40319\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.3894 - acc: 0.8297 - val_loss: 0.4629 - val_acc: 0.7715\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.40319\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.3682 - acc: 0.8416 - val_loss: 0.5239 - val_acc: 0.7565\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.40319\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.3454 - acc: 0.8499 - val_loss: 0.4222 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.40319\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.3137 - acc: 0.8676 - val_loss: 0.5899 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.40319\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2902 - acc: 0.8766 - val_loss: 0.4125 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.40319\n",
      "Epoch 13/50\n",
      " - 5s - loss: 0.2638 - acc: 0.8933 - val_loss: 0.4220 - val_acc: 0.8070\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.40319\n",
      "Epoch 00013: early stopping\n",
      "Attribute:  12\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7192 - acc: 0.5738 - val_loss: 0.6242 - val_acc: 0.6275\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62416, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6138 - acc: 0.6565 - val_loss: 0.5651 - val_acc: 0.6950\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62416 to 0.56513, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5485 - acc: 0.7139 - val_loss: 0.4594 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56513 to 0.45937, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.4815 - acc: 0.7713 - val_loss: 0.4829 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.45937\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.4409 - acc: 0.7912 - val_loss: 0.3955 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45937 to 0.39548, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.4051 - acc: 0.8189 - val_loss: 0.4642 - val_acc: 0.7830\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.39548\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.3829 - acc: 0.8282 - val_loss: 0.4456 - val_acc: 0.7965\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.39548\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.3552 - acc: 0.8436 - val_loss: 0.4335 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.39548\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.3311 - acc: 0.8566 - val_loss: 0.3977 - val_acc: 0.8195\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39548\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.3024 - acc: 0.8711 - val_loss: 0.6111 - val_acc: 0.7220\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39548\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.2835 - acc: 0.8792 - val_loss: 0.4036 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.39548\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2435 - acc: 0.8980 - val_loss: 0.4990 - val_acc: 0.7895\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.39548\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.2211 - acc: 0.9086 - val_loss: 0.5720 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.39548\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1915 - acc: 0.9208 - val_loss: 0.5582 - val_acc: 0.7805\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.39548\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1646 - acc: 0.9376 - val_loss: 0.6159 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.39548\n",
      "Epoch 00015: early stopping\n",
      "Attribute:  13\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6566 - acc: 0.6611 - val_loss: 0.4710 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47097, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.5145 - acc: 0.7476 - val_loss: 0.5174 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47097\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.4567 - acc: 0.7900 - val_loss: 0.4626 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47097 to 0.46263, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.4120 - acc: 0.8151 - val_loss: 0.3863 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46263 to 0.38631, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.3793 - acc: 0.8346 - val_loss: 0.3913 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38631\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.3560 - acc: 0.8491 - val_loss: 0.4603 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38631\n",
      "Epoch 7/50\n",
      " - 5s - loss: 0.3105 - acc: 0.8719 - val_loss: 0.4143 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.38631\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2860 - acc: 0.8807 - val_loss: 0.4681 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.38631\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2548 - acc: 0.8946 - val_loss: 0.4497 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.38631\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.2164 - acc: 0.9093 - val_loss: 0.5649 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38631\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1817 - acc: 0.9279 - val_loss: 0.5320 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.38631\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1594 - acc: 0.9374 - val_loss: 0.5968 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.38631\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1355 - acc: 0.9450 - val_loss: 0.5238 - val_acc: 0.8135\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.38631\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1074 - acc: 0.9596 - val_loss: 0.7047 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.38631\n",
      "Epoch 00014: early stopping\n",
      "Attribute:  14\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.3902 - acc: 0.8267 - val_loss: 0.3871 - val_acc: 0.8280\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38707, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.3718 - acc: 0.8376 - val_loss: 0.3620 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38707 to 0.36203, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.3360 - acc: 0.8537 - val_loss: 0.3287 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36203 to 0.32865, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3144 - acc: 0.8658 - val_loss: 0.4338 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.32865\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2912 - acc: 0.8728 - val_loss: 0.4123 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.32865\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2640 - acc: 0.8909 - val_loss: 0.3915 - val_acc: 0.8150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32865\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2456 - acc: 0.8996 - val_loss: 0.3772 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.32865\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2213 - acc: 0.9120 - val_loss: 0.4603 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.32865\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.1954 - acc: 0.9212 - val_loss: 0.5984 - val_acc: 0.7540\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.32865\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1672 - acc: 0.9357 - val_loss: 0.4384 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32865\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1478 - acc: 0.9437 - val_loss: 0.5326 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32865\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1316 - acc: 0.9489 - val_loss: 0.6473 - val_acc: 0.7815\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.32865\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1098 - acc: 0.9561 - val_loss: 0.4890 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.32865\n",
      "Epoch 00013: early stopping\n",
      "Attribute:  15\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.3543 - acc: 0.8476 - val_loss: 0.2205 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22052, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.2035 - acc: 0.9152 - val_loss: 0.1644 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.22052 to 0.16444, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.1708 - acc: 0.9323 - val_loss: 0.1479 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16444 to 0.14794, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.1294 - acc: 0.9504 - val_loss: 0.1193 - val_acc: 0.9505\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14794 to 0.11930, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.1184 - acc: 0.9524 - val_loss: 0.1327 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11930\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.1031 - acc: 0.9587 - val_loss: 0.1183 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11930 to 0.11831, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.0866 - acc: 0.9701 - val_loss: 0.1175 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.11831 to 0.11748, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.0715 - acc: 0.9736 - val_loss: 0.1078 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11748 to 0.10783, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.0610 - acc: 0.9801 - val_loss: 0.1155 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10783\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.0513 - acc: 0.9828 - val_loss: 0.1062 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.10783 to 0.10621, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.0506 - acc: 0.9810 - val_loss: 0.1380 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.10621\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.0388 - acc: 0.9858 - val_loss: 0.1299 - val_acc: 0.9550\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.10621\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.0315 - acc: 0.9896 - val_loss: 0.1298 - val_acc: 0.9525\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.10621\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.0322 - acc: 0.9889 - val_loss: 0.1100 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.10621\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0273 - acc: 0.9915 - val_loss: 0.1283 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.10621\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0198 - acc: 0.9938 - val_loss: 0.1591 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10621\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0180 - acc: 0.9936 - val_loss: 0.1239 - val_acc: 0.9665\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10621\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.0189 - acc: 0.9948 - val_loss: 0.1458 - val_acc: 0.9560\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10621\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.0128 - acc: 0.9958 - val_loss: 0.1445 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10621\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.0146 - acc: 0.9945 - val_loss: 0.1536 - val_acc: 0.9555\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10621\n",
      "Epoch 00020: early stopping\n",
      "Attribute:  16\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6592 - acc: 0.6784 - val_loss: 0.5711 - val_acc: 0.6965\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57113, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4780 - acc: 0.7660 - val_loss: 0.4558 - val_acc: 0.7845\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57113 to 0.45578, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.4084 - acc: 0.8110 - val_loss: 0.3765 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45578 to 0.37648, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3566 - acc: 0.8365 - val_loss: 0.2993 - val_acc: 0.8745\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37648 to 0.29935, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.3123 - acc: 0.8644 - val_loss: 0.3420 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29935\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2933 - acc: 0.8735 - val_loss: 0.2793 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29935 to 0.27931, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2647 - acc: 0.8886 - val_loss: 0.3447 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.27931\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2431 - acc: 0.8994 - val_loss: 0.3191 - val_acc: 0.8685\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.27931\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2168 - acc: 0.9133 - val_loss: 0.3693 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.27931\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1889 - acc: 0.9221 - val_loss: 0.3228 - val_acc: 0.8685\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27931\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1667 - acc: 0.9341 - val_loss: 0.3758 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27931\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1497 - acc: 0.9407 - val_loss: 0.3590 - val_acc: 0.8745\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.27931\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1280 - acc: 0.9535 - val_loss: 0.4140 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27931\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1117 - acc: 0.9582 - val_loss: 0.4032 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.27931\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0948 - acc: 0.9641 - val_loss: 0.3578 - val_acc: 0.8810\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.27931\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0803 - acc: 0.9700 - val_loss: 0.3983 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.27931\n",
      "Epoch 00016: early stopping\n",
      "Attribute:  17\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.4662 - acc: 0.7825 - val_loss: 0.3506 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35059, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.2818 - acc: 0.8791 - val_loss: 0.3535 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35059\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.2448 - acc: 0.8994 - val_loss: 0.2552 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35059 to 0.25521, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.2040 - acc: 0.9168 - val_loss: 0.2519 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25521 to 0.25187, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.1807 - acc: 0.9274 - val_loss: 0.2467 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25187 to 0.24673, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.1595 - acc: 0.9363 - val_loss: 0.2270 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24673 to 0.22703, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.1330 - acc: 0.9462 - val_loss: 0.3281 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22703\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.1140 - acc: 0.9551 - val_loss: 0.3830 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22703\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.0946 - acc: 0.9656 - val_loss: 0.2654 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22703\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.0789 - acc: 0.9695 - val_loss: 0.2586 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22703\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.0748 - acc: 0.9700 - val_loss: 0.3431 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22703\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.0548 - acc: 0.9826 - val_loss: 0.3395 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22703\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.0494 - acc: 0.9835 - val_loss: 0.2915 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22703\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.0435 - acc: 0.9843 - val_loss: 0.4297 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22703\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0359 - acc: 0.9878 - val_loss: 0.3668 - val_acc: 0.9045\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.22703\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0350 - acc: 0.9891 - val_loss: 0.3800 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.22703\n",
      "Epoch 00016: early stopping\n",
      "Attribute:  18\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7234 - acc: 0.6539 - val_loss: 0.5287 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52865, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4936 - acc: 0.7630 - val_loss: 0.4511 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52865 to 0.45111, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.4284 - acc: 0.8069 - val_loss: 0.3989 - val_acc: 0.8195\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45111 to 0.39892, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3902 - acc: 0.8295 - val_loss: 0.3685 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39892 to 0.36854, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.3654 - acc: 0.8395 - val_loss: 0.3640 - val_acc: 0.8355\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36854 to 0.36400, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.3422 - acc: 0.8519 - val_loss: 0.3522 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36400 to 0.35221, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.3238 - acc: 0.8593 - val_loss: 0.3475 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35221 to 0.34753, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.3148 - acc: 0.8603 - val_loss: 0.3201 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.34753 to 0.32008, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2948 - acc: 0.8745 - val_loss: 0.3203 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.32008\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.2825 - acc: 0.8771 - val_loss: 0.3144 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.32008 to 0.31438, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.2768 - acc: 0.8786 - val_loss: 0.3210 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31438\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2612 - acc: 0.8896 - val_loss: 0.3198 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31438\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.2517 - acc: 0.8961 - val_loss: 0.3139 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.31438 to 0.31391, saving model to model.weights.best.hdf5\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.2427 - acc: 0.8989 - val_loss: 0.3286 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31391\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.2306 - acc: 0.9006 - val_loss: 0.3469 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31391\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.2226 - acc: 0.9088 - val_loss: 0.3216 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31391\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.2054 - acc: 0.9165 - val_loss: 0.3334 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31391\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.1936 - acc: 0.9173 - val_loss: 0.3577 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31391\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.1878 - acc: 0.9236 - val_loss: 0.3581 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31391\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.1747 - acc: 0.9286 - val_loss: 0.3637 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.31391\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.1577 - acc: 0.9387 - val_loss: 0.3846 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.31391\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.1447 - acc: 0.9425 - val_loss: 0.3812 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.31391\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.1346 - acc: 0.9477 - val_loss: 0.4102 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.31391\n",
      "Epoch 00023: early stopping\n",
      "Attribute:  19\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7188 - acc: 0.5939 - val_loss: 0.6554 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65540, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6131 - acc: 0.6690 - val_loss: 0.6153 - val_acc: 0.6825\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65540 to 0.61532, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5486 - acc: 0.7233 - val_loss: 0.4911 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61532 to 0.49111, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.4992 - acc: 0.7604 - val_loss: 0.4365 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49111 to 0.43655, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.4510 - acc: 0.7875 - val_loss: 0.4407 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43655\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.4198 - acc: 0.8044 - val_loss: 0.4141 - val_acc: 0.8175\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43655 to 0.41409, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.3936 - acc: 0.8226 - val_loss: 0.4217 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.41409\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.3726 - acc: 0.8318 - val_loss: 0.5022 - val_acc: 0.7565\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.41409\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.3421 - acc: 0.8527 - val_loss: 0.4397 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.41409\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.2981 - acc: 0.8735 - val_loss: 0.4270 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.41409\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.2629 - acc: 0.8893 - val_loss: 0.4795 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.41409\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2357 - acc: 0.9036 - val_loss: 0.5513 - val_acc: 0.7515\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.41409\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.2094 - acc: 0.9113 - val_loss: 0.5615 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.41409\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1791 - acc: 0.9300 - val_loss: 0.4613 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.41409\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1455 - acc: 0.9432 - val_loss: 0.5140 - val_acc: 0.8070\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.41409\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.1240 - acc: 0.9524 - val_loss: 0.5531 - val_acc: 0.7985\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.41409\n",
      "Epoch 00016: early stopping\n",
      "Attribute:  20\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6276 - acc: 0.6796 - val_loss: 0.4807 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48069, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4365 - acc: 0.7960 - val_loss: 0.3428 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48069 to 0.34277, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.3557 - acc: 0.8420 - val_loss: 0.3839 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34277\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3010 - acc: 0.8666 - val_loss: 0.2663 - val_acc: 0.8850\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34277 to 0.26629, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2492 - acc: 0.8939 - val_loss: 0.3505 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26629\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2075 - acc: 0.9177 - val_loss: 0.2146 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26629 to 0.21462, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.1741 - acc: 0.9315 - val_loss: 0.2884 - val_acc: 0.8830\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21462\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.1609 - acc: 0.9351 - val_loss: 0.1979 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.21462 to 0.19791, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.1466 - acc: 0.9392 - val_loss: 0.2202 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.19791\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1327 - acc: 0.9497 - val_loss: 0.2670 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.19791\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1146 - acc: 0.9571 - val_loss: 0.1932 - val_acc: 0.9255\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.19791 to 0.19318, saving model to model.weights.best.hdf5\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.0945 - acc: 0.9653 - val_loss: 0.2610 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19318\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.0944 - acc: 0.9648 - val_loss: 0.1804 - val_acc: 0.9230\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.19318 to 0.18041, saving model to model.weights.best.hdf5\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.0810 - acc: 0.9685 - val_loss: 0.1952 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18041\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0725 - acc: 0.9726 - val_loss: 0.2509 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.18041\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0620 - acc: 0.9764 - val_loss: 0.2762 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18041\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0532 - acc: 0.9793 - val_loss: 0.2829 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18041\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.0516 - acc: 0.9798 - val_loss: 0.2651 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18041\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.0362 - acc: 0.9875 - val_loss: 0.2999 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18041\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.0365 - acc: 0.9878 - val_loss: 0.3522 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18041\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.0323 - acc: 0.9900 - val_loss: 0.2954 - val_acc: 0.9215\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.18041\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.0323 - acc: 0.9886 - val_loss: 0.2087 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.18041\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.0252 - acc: 0.9914 - val_loss: 0.2718 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.18041\n",
      "Epoch 00023: early stopping\n",
      "Attribute:  21\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.8318 - acc: 0.5465 - val_loss: 0.6534 - val_acc: 0.6355\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65341, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6178 - acc: 0.6595 - val_loss: 0.5907 - val_acc: 0.6810\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65341 to 0.59071, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5499 - acc: 0.7226 - val_loss: 0.5448 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59071 to 0.54478, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.5236 - acc: 0.7428 - val_loss: 0.4956 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54478 to 0.49564, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.4866 - acc: 0.7705 - val_loss: 0.4690 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49564 to 0.46900, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.4588 - acc: 0.7846 - val_loss: 0.4601 - val_acc: 0.7895\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.46900 to 0.46006, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.4304 - acc: 0.8011 - val_loss: 0.4297 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.46006 to 0.42973, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.4055 - acc: 0.8154 - val_loss: 0.4080 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42973 to 0.40802, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.3706 - acc: 0.8383 - val_loss: 0.4173 - val_acc: 0.8150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.40802\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.3497 - acc: 0.8436 - val_loss: 0.3687 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.40802 to 0.36872, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.3186 - acc: 0.8581 - val_loss: 0.3686 - val_acc: 0.8380\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36872 to 0.36860, saving model to model.weights.best.hdf5\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2843 - acc: 0.8807 - val_loss: 0.3682 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.36860 to 0.36820, saving model to model.weights.best.hdf5\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.2658 - acc: 0.8880 - val_loss: 0.3528 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.36820 to 0.35284, saving model to model.weights.best.hdf5\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.2385 - acc: 0.9041 - val_loss: 0.3489 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.35284 to 0.34893, saving model to model.weights.best.hdf5\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.2212 - acc: 0.9097 - val_loss: 0.3446 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.34893 to 0.34456, saving model to model.weights.best.hdf5\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.1941 - acc: 0.9238 - val_loss: 0.3645 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34456\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.1694 - acc: 0.9328 - val_loss: 0.3776 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.34456\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.1573 - acc: 0.9387 - val_loss: 0.3673 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.34456\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.1398 - acc: 0.9472 - val_loss: 0.3844 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.34456\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.1155 - acc: 0.9557 - val_loss: 0.3959 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.34456\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.1015 - acc: 0.9616 - val_loss: 0.4645 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.34456\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.0910 - acc: 0.9647 - val_loss: 0.4348 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.34456\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.0709 - acc: 0.9738 - val_loss: 0.4708 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.34456\n",
      "Epoch 24/50\n",
      " - 4s - loss: 0.0677 - acc: 0.9783 - val_loss: 0.4870 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.34456\n",
      "Epoch 25/50\n",
      " - 4s - loss: 0.0658 - acc: 0.9759 - val_loss: 0.5171 - val_acc: 0.8330\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.34456\n",
      "Epoch 00025: early stopping\n",
      "Attribute:  22\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6838 - acc: 0.6591 - val_loss: 0.6451 - val_acc: 0.6435\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64510, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4838 - acc: 0.7645 - val_loss: 0.4249 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64510 to 0.42493, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.4105 - acc: 0.8099 - val_loss: 0.5287 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.42493\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3710 - acc: 0.8343 - val_loss: 0.3962 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42493 to 0.39617, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.3295 - acc: 0.8563 - val_loss: 0.4313 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.39617\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2947 - acc: 0.8739 - val_loss: 0.4321 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.39617\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2780 - acc: 0.8830 - val_loss: 0.3816 - val_acc: 0.8330\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.39617 to 0.38160, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2500 - acc: 0.8965 - val_loss: 0.4896 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.38160\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2229 - acc: 0.9089 - val_loss: 0.4789 - val_acc: 0.7890\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.38160\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1971 - acc: 0.9231 - val_loss: 0.3720 - val_acc: 0.8420\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.38160 to 0.37196, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1778 - acc: 0.9305 - val_loss: 0.4295 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37196\n",
      "Epoch 12/50\n",
      " - 5s - loss: 0.1523 - acc: 0.9385 - val_loss: 0.3144 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.37196 to 0.31435, saving model to model.weights.best.hdf5\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1325 - acc: 0.9500 - val_loss: 0.4709 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31435\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1155 - acc: 0.9565 - val_loss: 0.5994 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31435\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1016 - acc: 0.9630 - val_loss: 0.5937 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31435\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0903 - acc: 0.9674 - val_loss: 0.4618 - val_acc: 0.8470\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31435\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0741 - acc: 0.9731 - val_loss: 0.5005 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31435\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.0778 - acc: 0.9708 - val_loss: 0.4786 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31435\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.0627 - acc: 0.9779 - val_loss: 0.6933 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31435\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.0586 - acc: 0.9793 - val_loss: 0.5615 - val_acc: 0.8420\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.31435\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.0486 - acc: 0.9843 - val_loss: 0.7682 - val_acc: 0.8010\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.31435\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.0484 - acc: 0.9823 - val_loss: 0.7388 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.31435\n",
      "Epoch 00022: early stopping\n",
      "Attribute:  23\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7971 - acc: 0.5374 - val_loss: 0.6901 - val_acc: 0.5515\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69015, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6645 - acc: 0.5965 - val_loss: 0.6905 - val_acc: 0.5915\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69015\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.6325 - acc: 0.6487 - val_loss: 0.6242 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69015 to 0.62416, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.5881 - acc: 0.6830 - val_loss: 0.5994 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62416 to 0.59945, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.5595 - acc: 0.7161 - val_loss: 0.5538 - val_acc: 0.7155\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.59945 to 0.55384, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.5083 - acc: 0.7542 - val_loss: 0.5626 - val_acc: 0.7145\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.55384\n",
      "Epoch 7/50\n",
      " - 5s - loss: 0.4856 - acc: 0.7684 - val_loss: 0.5647 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.55384\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.4463 - acc: 0.7903 - val_loss: 0.6027 - val_acc: 0.7010\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.55384\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.4029 - acc: 0.8143 - val_loss: 0.5798 - val_acc: 0.7135\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.55384\n",
      "Epoch 10/50\n",
      " - 5s - loss: 0.3646 - acc: 0.8381 - val_loss: 0.5741 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.55384\n",
      "Epoch 11/50\n",
      " - 5s - loss: 0.3355 - acc: 0.8529 - val_loss: 0.6451 - val_acc: 0.6980\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.55384\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2899 - acc: 0.8711 - val_loss: 0.6888 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.55384\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.2394 - acc: 0.9002 - val_loss: 0.7311 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.55384\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.2016 - acc: 0.9206 - val_loss: 0.7969 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.55384\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1661 - acc: 0.9335 - val_loss: 0.8000 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.55384\n",
      "Epoch 00015: early stopping\n",
      "Attribute:  24\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6485 - acc: 0.6409 - val_loss: 0.5357 - val_acc: 0.7365\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53566, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.5151 - acc: 0.7515 - val_loss: 0.4448 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53566 to 0.44475, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.4465 - acc: 0.7913 - val_loss: 0.3963 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44475 to 0.39626, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.4057 - acc: 0.8156 - val_loss: 0.3645 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39626 to 0.36447, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.3685 - acc: 0.8304 - val_loss: 0.3482 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36447 to 0.34821, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.3428 - acc: 0.8499 - val_loss: 0.3424 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34821 to 0.34236, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.3177 - acc: 0.8557 - val_loss: 0.3393 - val_acc: 0.8450\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.34236 to 0.33928, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.3083 - acc: 0.8675 - val_loss: 0.3364 - val_acc: 0.8440\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.33928 to 0.33643, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2921 - acc: 0.8751 - val_loss: 0.3306 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33643 to 0.33059, saving model to model.weights.best.hdf5\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.2780 - acc: 0.8791 - val_loss: 0.3293 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.33059 to 0.32932, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.2523 - acc: 0.8895 - val_loss: 0.3212 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.32932 to 0.32120, saving model to model.weights.best.hdf5\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2395 - acc: 0.8947 - val_loss: 0.3171 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.32120 to 0.31709, saving model to model.weights.best.hdf5\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.2373 - acc: 0.8966 - val_loss: 0.3617 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31709\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.2136 - acc: 0.9100 - val_loss: 0.3172 - val_acc: 0.8660\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31709\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1890 - acc: 0.9216 - val_loss: 0.3357 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31709\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.1750 - acc: 0.9324 - val_loss: 0.3720 - val_acc: 0.8470\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31709\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.1548 - acc: 0.9379 - val_loss: 0.3727 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31709\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.1401 - acc: 0.9455 - val_loss: 0.3699 - val_acc: 0.8450\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31709\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.1259 - acc: 0.9500 - val_loss: 0.3947 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31709\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.1151 - acc: 0.9565 - val_loss: 0.4022 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.31709\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.1040 - acc: 0.9614 - val_loss: 0.4157 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.31709\n",
      "Epoch 22/50\n",
      " - 4s - loss: 0.0872 - acc: 0.9678 - val_loss: 0.4518 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.31709\n",
      "Epoch 00022: early stopping\n",
      "Attribute:  25\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7704 - acc: 0.5287 - val_loss: 0.6857 - val_acc: 0.5630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68569, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6821 - acc: 0.5726 - val_loss: 0.6769 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68569 to 0.67690, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.6718 - acc: 0.5874 - val_loss: 0.6696 - val_acc: 0.5940\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67690 to 0.66956, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.6630 - acc: 0.6018 - val_loss: 0.6665 - val_acc: 0.6075\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.66956 to 0.66655, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.6532 - acc: 0.6107 - val_loss: 0.6631 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.66655 to 0.66310, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.6442 - acc: 0.6285 - val_loss: 0.6641 - val_acc: 0.5950\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.66310\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.6313 - acc: 0.6415 - val_loss: 0.6656 - val_acc: 0.5980\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.66310\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.6212 - acc: 0.6558 - val_loss: 0.6573 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.66310 to 0.65735, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.6055 - acc: 0.6679 - val_loss: 0.6753 - val_acc: 0.6035\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.65735\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.5888 - acc: 0.6835 - val_loss: 0.6654 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.65735\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.5706 - acc: 0.6953 - val_loss: 0.6914 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.65735\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.5509 - acc: 0.7142 - val_loss: 0.6945 - val_acc: 0.5925\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.65735\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.5169 - acc: 0.7406 - val_loss: 0.6868 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.65735\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.4848 - acc: 0.7622 - val_loss: 0.7169 - val_acc: 0.6070\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.65735\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.4507 - acc: 0.7839 - val_loss: 0.7526 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.65735\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.4194 - acc: 0.8066 - val_loss: 0.7923 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.65735\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.3687 - acc: 0.8294 - val_loss: 0.8227 - val_acc: 0.5865\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.65735\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.3240 - acc: 0.8556 - val_loss: 0.8956 - val_acc: 0.5970\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.65735\n",
      "Epoch 00018: early stopping\n",
      "Attribute:  26\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.4910 - acc: 0.7551 - val_loss: 0.3797 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37971, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.3236 - acc: 0.8686 - val_loss: 0.2134 - val_acc: 0.9070\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37971 to 0.21336, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.2733 - acc: 0.8866 - val_loss: 0.4131 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21336\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.2790 - acc: 0.8859 - val_loss: 0.2525 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21336\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2548 - acc: 0.8984 - val_loss: 0.2210 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21336\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2476 - acc: 0.9011 - val_loss: 0.1808 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.21336 to 0.18081, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2461 - acc: 0.8966 - val_loss: 0.2273 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.18081\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2148 - acc: 0.9125 - val_loss: 0.2408 - val_acc: 0.8980\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.18081\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2006 - acc: 0.9211 - val_loss: 0.3403 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.18081\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1895 - acc: 0.9259 - val_loss: 0.2624 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.18081\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1739 - acc: 0.9310 - val_loss: 0.2897 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.18081\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1564 - acc: 0.9392 - val_loss: 0.2052 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.18081\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1479 - acc: 0.9424 - val_loss: 0.3661 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.18081\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1271 - acc: 0.9537 - val_loss: 0.2253 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18081\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1118 - acc: 0.9580 - val_loss: 0.5597 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.18081\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.1001 - acc: 0.9621 - val_loss: 0.3323 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18081\n",
      "Epoch 00016: early stopping\n",
      "Attribute:  27\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7739 - acc: 0.5473 - val_loss: 0.6762 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67616, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6636 - acc: 0.5964 - val_loss: 0.6610 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67616 to 0.66103, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.6501 - acc: 0.6201 - val_loss: 0.6589 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.66103 to 0.65893, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.6408 - acc: 0.6319 - val_loss: 0.6595 - val_acc: 0.6145\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.65893\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.6352 - acc: 0.6366 - val_loss: 0.6543 - val_acc: 0.6040\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65893 to 0.65427, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.6235 - acc: 0.6530 - val_loss: 0.6593 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.65427\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.6153 - acc: 0.6595 - val_loss: 0.6599 - val_acc: 0.6075\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.65427\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.6034 - acc: 0.6695 - val_loss: 0.6604 - val_acc: 0.6045\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.65427\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.5908 - acc: 0.6884 - val_loss: 0.6664 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.65427\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.5741 - acc: 0.6949 - val_loss: 0.6763 - val_acc: 0.6030\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.65427\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.5507 - acc: 0.7160 - val_loss: 0.6848 - val_acc: 0.5905\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.65427\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.5270 - acc: 0.7332 - val_loss: 0.6928 - val_acc: 0.5915\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.65427\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.5036 - acc: 0.7462 - val_loss: 0.7157 - val_acc: 0.5865\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.65427\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.4707 - acc: 0.7745 - val_loss: 0.7466 - val_acc: 0.5925\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.65427\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.4310 - acc: 0.7896 - val_loss: 0.7572 - val_acc: 0.5975\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.65427\n",
      "Epoch 00015: early stopping\n",
      "Attribute:  28\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6982 - acc: 0.5355 - val_loss: 0.6734 - val_acc: 0.5995\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67343, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.5929 - acc: 0.6783 - val_loss: 0.5225 - val_acc: 0.7425\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67343 to 0.52253, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.4923 - acc: 0.7580 - val_loss: 0.4649 - val_acc: 0.7715\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52253 to 0.46494, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.4393 - acc: 0.7936 - val_loss: 0.4566 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46494 to 0.45661, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.3957 - acc: 0.8205 - val_loss: 0.4613 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.45661\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.3509 - acc: 0.8484 - val_loss: 0.4320 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.45661 to 0.43205, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.3048 - acc: 0.8687 - val_loss: 0.4617 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43205\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2674 - acc: 0.8904 - val_loss: 0.4442 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43205\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2374 - acc: 0.9015 - val_loss: 0.4741 - val_acc: 0.7965\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43205\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1893 - acc: 0.9215 - val_loss: 0.5068 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43205\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1546 - acc: 0.9415 - val_loss: 0.5256 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43205\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1329 - acc: 0.9464 - val_loss: 0.5723 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43205\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1206 - acc: 0.9537 - val_loss: 0.6549 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43205\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.0949 - acc: 0.9660 - val_loss: 0.6789 - val_acc: 0.7715\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43205\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0803 - acc: 0.9708 - val_loss: 0.7328 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43205\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0654 - acc: 0.9758 - val_loss: 0.7174 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.43205\n",
      "Epoch 00016: early stopping\n",
      "Attribute:  29\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6528 - acc: 0.6672 - val_loss: 0.4595 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45950, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4474 - acc: 0.7960 - val_loss: 0.3652 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45950 to 0.36524, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.3874 - acc: 0.8324 - val_loss: 0.4029 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36524\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3565 - acc: 0.8450 - val_loss: 0.4065 - val_acc: 0.8175\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36524\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.3265 - acc: 0.8647 - val_loss: 0.3107 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36524 to 0.31073, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.3015 - acc: 0.8776 - val_loss: 0.3508 - val_acc: 0.8420\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31073\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2840 - acc: 0.8824 - val_loss: 0.3527 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31073\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2659 - acc: 0.8870 - val_loss: 0.3380 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31073\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.2598 - acc: 0.8894 - val_loss: 0.3278 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31073\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.2256 - acc: 0.9073 - val_loss: 0.5052 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31073\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.2219 - acc: 0.9108 - val_loss: 0.3197 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31073\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1985 - acc: 0.9204 - val_loss: 0.3264 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31073\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1711 - acc: 0.9342 - val_loss: 0.4451 - val_acc: 0.8315\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31073\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1435 - acc: 0.9469 - val_loss: 0.4046 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31073\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.1343 - acc: 0.9494 - val_loss: 0.4106 - val_acc: 0.8420\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31073\n",
      "Epoch 00015: early stopping\n",
      "Attribute:  30\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7018 - acc: 0.6388 - val_loss: 0.5391 - val_acc: 0.7410\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53909, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4083 - acc: 0.8120 - val_loss: 0.3865 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53909 to 0.38648, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.3144 - acc: 0.8653 - val_loss: 0.2576 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38648 to 0.25762, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.2801 - acc: 0.8786 - val_loss: 0.2734 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25762\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2574 - acc: 0.8902 - val_loss: 0.3292 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25762\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2266 - acc: 0.9060 - val_loss: 0.3109 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25762\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2048 - acc: 0.9176 - val_loss: 0.2592 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25762\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.1882 - acc: 0.9269 - val_loss: 0.2896 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25762\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.1788 - acc: 0.9296 - val_loss: 0.3559 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25762\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1512 - acc: 0.9419 - val_loss: 0.2527 - val_acc: 0.8970\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.25762 to 0.25274, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1362 - acc: 0.9487 - val_loss: 0.3537 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25274\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1152 - acc: 0.9579 - val_loss: 0.2789 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25274\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.0986 - acc: 0.9628 - val_loss: 0.3479 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.25274\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1059 - acc: 0.9605 - val_loss: 0.3093 - val_acc: 0.8835\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.25274\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0861 - acc: 0.9685 - val_loss: 0.3370 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.25274\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0738 - acc: 0.9728 - val_loss: 0.3227 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.25274\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0591 - acc: 0.9801 - val_loss: 0.3520 - val_acc: 0.8870\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.25274\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.0602 - acc: 0.9786 - val_loss: 0.3530 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.25274\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.0498 - acc: 0.9838 - val_loss: 0.3587 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.25274\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.0418 - acc: 0.9860 - val_loss: 0.4709 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.25274\n",
      "Epoch 00020: early stopping\n",
      "Attribute:  31\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.8420 - acc: 0.5430 - val_loss: 0.7448 - val_acc: 0.5075\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74478, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6007 - acc: 0.6689 - val_loss: 0.5181 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.74478 to 0.51814, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5064 - acc: 0.7509 - val_loss: 0.4739 - val_acc: 0.7745\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51814 to 0.47387, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 5s - loss: 0.4500 - acc: 0.7859 - val_loss: 0.4247 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.47387 to 0.42471, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.3994 - acc: 0.8206 - val_loss: 0.3626 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42471 to 0.36259, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.3713 - acc: 0.8317 - val_loss: 0.3658 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36259\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.3412 - acc: 0.8489 - val_loss: 0.3550 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.36259 to 0.35499, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.3146 - acc: 0.8595 - val_loss: 0.3270 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35499 to 0.32702, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 5s - loss: 0.3014 - acc: 0.8681 - val_loss: 0.3428 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.32702\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.2882 - acc: 0.8755 - val_loss: 0.3324 - val_acc: 0.8690\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32702\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.2662 - acc: 0.8875 - val_loss: 0.3454 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32702\n",
      "Epoch 12/50\n",
      " - 5s - loss: 0.2517 - acc: 0.8920 - val_loss: 0.3373 - val_acc: 0.8620\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.32702\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.2274 - acc: 0.9044 - val_loss: 0.3448 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.32702\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.2115 - acc: 0.9134 - val_loss: 0.3511 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.32702\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.2009 - acc: 0.9175 - val_loss: 0.3690 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.32702\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.1871 - acc: 0.9268 - val_loss: 0.3701 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.32702\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.1706 - acc: 0.9316 - val_loss: 0.3745 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.32702\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.1569 - acc: 0.9411 - val_loss: 0.3856 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.32702\n",
      "Epoch 00018: early stopping\n",
      "Attribute:  32\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7505 - acc: 0.5301 - val_loss: 0.7095 - val_acc: 0.4630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70949, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6798 - acc: 0.5556 - val_loss: 0.7282 - val_acc: 0.4210\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.70949\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.6678 - acc: 0.5806 - val_loss: 0.6976 - val_acc: 0.5125\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.70949 to 0.69762, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.6535 - acc: 0.6092 - val_loss: 0.6428 - val_acc: 0.6310\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69762 to 0.64283, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.6354 - acc: 0.6349 - val_loss: 0.7206 - val_acc: 0.4755\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.64283\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.6053 - acc: 0.6505 - val_loss: 0.6663 - val_acc: 0.5925\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.64283\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.5860 - acc: 0.6731 - val_loss: 0.6956 - val_acc: 0.5665\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.64283\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.5523 - acc: 0.7139 - val_loss: 0.7574 - val_acc: 0.5105\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.64283\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.5219 - acc: 0.7325 - val_loss: 0.7085 - val_acc: 0.5770\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.64283\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.4772 - acc: 0.7580 - val_loss: 0.7114 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.64283\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.4409 - acc: 0.7812 - val_loss: 0.7382 - val_acc: 0.6185\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.64283\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.3982 - acc: 0.8101 - val_loss: 0.7323 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.64283\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.3733 - acc: 0.8207 - val_loss: 0.7864 - val_acc: 0.6345\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.64283\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.3378 - acc: 0.8420 - val_loss: 0.7916 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.64283\n",
      "Epoch 00014: early stopping\n",
      "Attribute:  33\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6862 - acc: 0.5625 - val_loss: 0.6416 - val_acc: 0.6435\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64159, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6306 - acc: 0.6486 - val_loss: 0.6167 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64159 to 0.61673, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5876 - acc: 0.6858 - val_loss: 0.5806 - val_acc: 0.7065\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61673 to 0.58056, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 5s - loss: 0.5600 - acc: 0.7099 - val_loss: 0.5791 - val_acc: 0.7095\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58056 to 0.57906, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.5400 - acc: 0.7276 - val_loss: 0.5651 - val_acc: 0.7185\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.57906 to 0.56515, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.5196 - acc: 0.7410 - val_loss: 0.5616 - val_acc: 0.7135\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56515 to 0.56158, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 5s - loss: 0.4934 - acc: 0.7571 - val_loss: 0.5534 - val_acc: 0.7230\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.56158 to 0.55336, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 5s - loss: 0.4758 - acc: 0.7731 - val_loss: 0.5512 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.55336 to 0.55121, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 5s - loss: 0.4439 - acc: 0.7909 - val_loss: 0.5674 - val_acc: 0.7245\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.55121\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.4201 - acc: 0.8059 - val_loss: 0.5662 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.55121\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.3811 - acc: 0.8296 - val_loss: 0.5779 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.55121\n",
      "Epoch 12/50\n",
      " - 5s - loss: 0.3504 - acc: 0.8440 - val_loss: 0.5933 - val_acc: 0.7235\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.55121\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.3181 - acc: 0.8610 - val_loss: 0.6372 - val_acc: 0.7210\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.55121\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.2760 - acc: 0.8861 - val_loss: 0.6494 - val_acc: 0.7190\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.55121\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.2415 - acc: 0.9031 - val_loss: 0.7321 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.55121\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.2211 - acc: 0.9141 - val_loss: 0.7842 - val_acc: 0.7185\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.55121\n",
      "Epoch 17/50\n",
      " - 5s - loss: 0.1754 - acc: 0.9324 - val_loss: 0.7828 - val_acc: 0.7165\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.55121\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.1409 - acc: 0.9499 - val_loss: 0.8142 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.55121\n",
      "Epoch 00018: early stopping\n",
      "Attribute:  34\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6453 - acc: 0.6400 - val_loss: 0.5853 - val_acc: 0.6975\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58531, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.5768 - acc: 0.6944 - val_loss: 0.5576 - val_acc: 0.7195\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58531 to 0.55757, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5378 - acc: 0.7279 - val_loss: 0.5462 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55757 to 0.54617, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.5078 - acc: 0.7455 - val_loss: 0.5395 - val_acc: 0.7175\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54617 to 0.53949, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.4704 - acc: 0.7712 - val_loss: 0.5328 - val_acc: 0.7365\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53949 to 0.53280, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.4272 - acc: 0.8016 - val_loss: 0.5398 - val_acc: 0.7335\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53280\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.3977 - acc: 0.8139 - val_loss: 0.5350 - val_acc: 0.7455\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53280\n",
      "Epoch 8/50\n",
      " - 5s - loss: 0.3541 - acc: 0.8407 - val_loss: 0.5508 - val_acc: 0.7455\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.53280\n",
      "Epoch 9/50\n",
      " - 5s - loss: 0.3165 - acc: 0.8646 - val_loss: 0.5601 - val_acc: 0.7455\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53280\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.2743 - acc: 0.8821 - val_loss: 0.5889 - val_acc: 0.7430\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.53280\n",
      "Epoch 11/50\n",
      " - 5s - loss: 0.2337 - acc: 0.9058 - val_loss: 0.6138 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.53280\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.2056 - acc: 0.9193 - val_loss: 0.6598 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.53280\n",
      "Epoch 13/50\n",
      " - 5s - loss: 0.1775 - acc: 0.9325 - val_loss: 0.6903 - val_acc: 0.7410\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.53280\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1499 - acc: 0.9414 - val_loss: 0.7166 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.53280\n",
      "Epoch 15/50\n",
      " - 5s - loss: 0.1296 - acc: 0.9515 - val_loss: 0.7706 - val_acc: 0.7425\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.53280\n",
      "Epoch 00015: early stopping\n",
      "Attribute:  35\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.5349 - acc: 0.7392 - val_loss: 0.2222 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22220, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 5s - loss: 0.2523 - acc: 0.8981 - val_loss: 0.1849 - val_acc: 0.9270\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.22220 to 0.18488, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 5s - loss: 0.1896 - acc: 0.9239 - val_loss: 0.1821 - val_acc: 0.9330\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.18488 to 0.18212, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 5s - loss: 0.1551 - acc: 0.9391 - val_loss: 0.1807 - val_acc: 0.9275\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.18212 to 0.18072, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.1301 - acc: 0.9494 - val_loss: 0.1709 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.18072 to 0.17095, saving model to model.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.1090 - acc: 0.9585 - val_loss: 0.2643 - val_acc: 0.8975\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.17095\n",
      "Epoch 7/50\n",
      " - 5s - loss: 0.0995 - acc: 0.9624 - val_loss: 0.2260 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.17095\n",
      "Epoch 8/50\n",
      " - 5s - loss: 0.0908 - acc: 0.9665 - val_loss: 0.1426 - val_acc: 0.9505\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.17095 to 0.14257, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 5s - loss: 0.0724 - acc: 0.9750 - val_loss: 0.1622 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14257\n",
      "Epoch 10/50\n",
      " - 5s - loss: 0.0571 - acc: 0.9783 - val_loss: 0.1745 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14257\n",
      "Epoch 11/50\n",
      " - 5s - loss: 0.0419 - acc: 0.9858 - val_loss: 0.1979 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14257\n",
      "Epoch 12/50\n",
      " - 5s - loss: 0.0391 - acc: 0.9864 - val_loss: 0.1588 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14257\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.0335 - acc: 0.9891 - val_loss: 0.2172 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14257\n",
      "Epoch 14/50\n",
      " - 5s - loss: 0.0285 - acc: 0.9905 - val_loss: 0.2355 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.14257\n",
      "Epoch 15/50\n",
      " - 5s - loss: 0.0239 - acc: 0.9929 - val_loss: 0.1901 - val_acc: 0.9460\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14257\n",
      "Epoch 16/50\n",
      " - 5s - loss: 0.0152 - acc: 0.9958 - val_loss: 0.2322 - val_acc: 0.9390\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.14257\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0138 - acc: 0.9959 - val_loss: 0.2464 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14257\n",
      "Epoch 18/50\n",
      " - 5s - loss: 0.0170 - acc: 0.9944 - val_loss: 0.2097 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14257\n",
      "Epoch 00018: early stopping\n",
      "Attribute:  36\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7569 - acc: 0.6711 - val_loss: 0.4978 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49782, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.4697 - acc: 0.7843 - val_loss: 0.4355 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49782 to 0.43555, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.3845 - acc: 0.8254 - val_loss: 0.3791 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43555 to 0.37914, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3137 - acc: 0.8641 - val_loss: 0.3055 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37914 to 0.30545, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2647 - acc: 0.8914 - val_loss: 0.3351 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30545\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2376 - acc: 0.9039 - val_loss: 0.3211 - val_acc: 0.8705\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30545\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2206 - acc: 0.9101 - val_loss: 0.2810 - val_acc: 0.8815\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30545 to 0.28103, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 5s - loss: 0.1923 - acc: 0.9219 - val_loss: 0.2780 - val_acc: 0.8850\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.28103 to 0.27799, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.1801 - acc: 0.9274 - val_loss: 0.2878 - val_acc: 0.8850\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.27799\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1614 - acc: 0.9321 - val_loss: 0.2772 - val_acc: 0.8920\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.27799 to 0.27722, saving model to model.weights.best.hdf5\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1334 - acc: 0.9479 - val_loss: 0.2720 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.27722 to 0.27202, saving model to model.weights.best.hdf5\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1213 - acc: 0.9554 - val_loss: 0.2830 - val_acc: 0.8965\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.27202\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1079 - acc: 0.9596 - val_loss: 0.3141 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27202\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.0843 - acc: 0.9690 - val_loss: 0.3252 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.27202\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0784 - acc: 0.9718 - val_loss: 0.3503 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.27202\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0632 - acc: 0.9775 - val_loss: 0.3633 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.27202\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0514 - acc: 0.9809 - val_loss: 0.3557 - val_acc: 0.8830\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.27202\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.0444 - acc: 0.9848 - val_loss: 0.3708 - val_acc: 0.8925\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.27202\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.0436 - acc: 0.9843 - val_loss: 0.3952 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.27202\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.0306 - acc: 0.9898 - val_loss: 0.4019 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.27202\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.0319 - acc: 0.9895 - val_loss: 0.3956 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.27202\n",
      "Epoch 00021: early stopping\n",
      "Attribute:  37\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.6692 - acc: 0.6624 - val_loss: 0.5889 - val_acc: 0.6680\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58888, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.5730 - acc: 0.6994 - val_loss: 0.5910 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.58888\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5554 - acc: 0.7100 - val_loss: 0.5632 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58888 to 0.56322, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.5310 - acc: 0.7266 - val_loss: 0.5626 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56322 to 0.56257, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.5218 - acc: 0.7321 - val_loss: 0.6026 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56257\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.5024 - acc: 0.7463 - val_loss: 0.5623 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56257 to 0.56232, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.4848 - acc: 0.7614 - val_loss: 0.5689 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.56232\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.4534 - acc: 0.7824 - val_loss: 0.5870 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.56232\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.4306 - acc: 0.7918 - val_loss: 0.6169 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.56232\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.4036 - acc: 0.8185 - val_loss: 0.6267 - val_acc: 0.6650\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.56232\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.3699 - acc: 0.8351 - val_loss: 0.6154 - val_acc: 0.6825\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.56232\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.3300 - acc: 0.8554 - val_loss: 0.6741 - val_acc: 0.6710\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.56232\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.3026 - acc: 0.8655 - val_loss: 0.6999 - val_acc: 0.6540\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.56232\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.2715 - acc: 0.8860 - val_loss: 0.7604 - val_acc: 0.6410\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.56232\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.2414 - acc: 0.9027 - val_loss: 0.7718 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.56232\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.2138 - acc: 0.9135 - val_loss: 0.8181 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.56232\n",
      "Epoch 00016: early stopping\n",
      "Attribute:  38\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.5455 - acc: 0.7411 - val_loss: 0.2599 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25994, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.2910 - acc: 0.8732 - val_loss: 0.2697 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25994\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.2688 - acc: 0.8856 - val_loss: 0.3318 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25994\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.2542 - acc: 0.8921 - val_loss: 0.2520 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25994 to 0.25204, saving model to model.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2365 - acc: 0.8972 - val_loss: 0.2771 - val_acc: 0.8730\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25204\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.2320 - acc: 0.9031 - val_loss: 0.3149 - val_acc: 0.8705\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25204\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.2166 - acc: 0.9082 - val_loss: 0.2762 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25204\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.2019 - acc: 0.9154 - val_loss: 0.2356 - val_acc: 0.8975\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25204 to 0.23564, saving model to model.weights.best.hdf5\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.1854 - acc: 0.9236 - val_loss: 0.2485 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.23564\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.1802 - acc: 0.9222 - val_loss: 0.2815 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.23564\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.1617 - acc: 0.9351 - val_loss: 0.2392 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.23564\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.1456 - acc: 0.9421 - val_loss: 0.2950 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.23564\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.1314 - acc: 0.9451 - val_loss: 0.3228 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.23564\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.1190 - acc: 0.9516 - val_loss: 0.2927 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.23564\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.0983 - acc: 0.9634 - val_loss: 0.2847 - val_acc: 0.8915\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.23564\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.0890 - acc: 0.9654 - val_loss: 0.2854 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.23564\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0810 - acc: 0.9700 - val_loss: 0.3510 - val_acc: 0.8835\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.23564\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.0683 - acc: 0.9753 - val_loss: 0.3933 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.23564\n",
      "Epoch 00018: early stopping\n",
      "Attribute:  39\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.7994 - acc: 0.5633 - val_loss: 0.6142 - val_acc: 0.7005\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61421, saving model to model.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.6033 - acc: 0.6676 - val_loss: 0.5324 - val_acc: 0.7585\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61421 to 0.53237, saving model to model.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.5750 - acc: 0.6897 - val_loss: 0.5239 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53237 to 0.52387, saving model to model.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.5562 - acc: 0.7086 - val_loss: 0.5508 - val_acc: 0.7435\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52387\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.5390 - acc: 0.7166 - val_loss: 0.5571 - val_acc: 0.7315\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52387\n",
      "Epoch 6/50\n",
      " - 4s - loss: 0.5215 - acc: 0.7339 - val_loss: 0.5105 - val_acc: 0.7610\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52387 to 0.51053, saving model to model.weights.best.hdf5\n",
      "Epoch 7/50\n",
      " - 4s - loss: 0.5134 - acc: 0.7442 - val_loss: 0.4492 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51053 to 0.44924, saving model to model.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.4903 - acc: 0.7511 - val_loss: 0.4731 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.44924\n",
      "Epoch 9/50\n",
      " - 4s - loss: 0.4825 - acc: 0.7614 - val_loss: 0.4366 - val_acc: 0.8115\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.44924 to 0.43656, saving model to model.weights.best.hdf5\n",
      "Epoch 10/50\n",
      " - 4s - loss: 0.4674 - acc: 0.7697 - val_loss: 0.4473 - val_acc: 0.7985\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43656\n",
      "Epoch 11/50\n",
      " - 4s - loss: 0.4536 - acc: 0.7821 - val_loss: 0.5003 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43656\n",
      "Epoch 12/50\n",
      " - 4s - loss: 0.4448 - acc: 0.7890 - val_loss: 0.4596 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43656\n",
      "Epoch 13/50\n",
      " - 4s - loss: 0.4342 - acc: 0.7961 - val_loss: 0.4829 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43656\n",
      "Epoch 14/50\n",
      " - 4s - loss: 0.4206 - acc: 0.8014 - val_loss: 0.4218 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.43656 to 0.42177, saving model to model.weights.best.hdf5\n",
      "Epoch 15/50\n",
      " - 4s - loss: 0.4095 - acc: 0.8095 - val_loss: 0.5144 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.42177\n",
      "Epoch 16/50\n",
      " - 4s - loss: 0.3911 - acc: 0.8214 - val_loss: 0.4832 - val_acc: 0.7935\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.42177\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.3667 - acc: 0.8358 - val_loss: 0.4479 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.42177\n",
      "Epoch 18/50\n",
      " - 4s - loss: 0.3509 - acc: 0.8379 - val_loss: 0.4971 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.42177\n",
      "Epoch 19/50\n",
      " - 4s - loss: 0.3461 - acc: 0.8497 - val_loss: 0.5343 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.42177\n",
      "Epoch 20/50\n",
      " - 4s - loss: 0.3124 - acc: 0.8654 - val_loss: 0.6875 - val_acc: 0.6815\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.42177\n",
      "Epoch 21/50\n",
      " - 4s - loss: 0.2907 - acc: 0.8749 - val_loss: 0.7160 - val_acc: 0.6630\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.42177\n",
      "Epoch 22/50\n",
      " - 5s - loss: 0.2726 - acc: 0.8874 - val_loss: 0.5853 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.42177\n",
      "Epoch 23/50\n",
      " - 4s - loss: 0.2508 - acc: 0.8978 - val_loss: 0.6103 - val_acc: 0.7420\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.42177\n",
      "Epoch 24/50\n",
      " - 4s - loss: 0.2296 - acc: 0.9075 - val_loss: 0.6140 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.42177\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "#카테고리별로 학습을 진행한다.\n",
    "#성능 검증을 위한 validation은 전체의 0.2퍼센트를 사용한다.\n",
    "#Epoch는 50을 주고 학습하되, 오버피팅을 방지하기 위해 EarlyStopping을 이용한다. 개선이 없다고 바로 종료하지 않고 개선이 없는 epoch를 얼마나 기다려 줄 것인 가를 지정한다. \n",
    "#만약 10이라고 지정하면 개선이 없는 epoch가 10번째 지속될 경우 학습을 종료한다.\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import scipy.misc\n",
    "\n",
    "for i in range(0,40):\n",
    "  X_train, y_train=prepare_train(train_files[i],train_labels[i])\n",
    "  print(\"Attribute: \",i)\n",
    "  checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "  stop=EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=1, mode='auto')\n",
    "  hist = model.fit(X_train, y_train, batch_size=200, epochs=50,\n",
    "          validation_split=0.2, callbacks=[checkpointer,stop], \n",
    "          verbose=2, shuffle=True) \n",
    "  model.load_weights('model.weights.best.hdf5')\n",
    "  model.save('model'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "YIZdet5acPv_",
    "outputId": "1d2a7af7-d2ea-4d05-daaa-799159e9c498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json\t\t\t model16.h5  model32.h5\n",
      "celeba-dataset.zip\t\t model17.h5  model33.h5\n",
      "img_align_celeba\t\t model18.h5  model34.h5\n",
      "img_align_celeba.zip\t\t model19.h5  model35.h5\n",
      "iu.jpeg\t\t\t\t model1.h5   model36.h5\n",
      "kaggle.json\t\t\t model20.h5  model37.h5\n",
      "list_attr_celeba.csv\t\t model21.h5  model38.h5\n",
      "list_bbox_celeba.csv\t\t model22.h5  model39.h5\n",
      "list_eval_partition.csv\t\t model23.h5  model3.h5\n",
      "list_landmarks_align_celeba.csv  model24.h5  model4.h5\n",
      "mmod_human_face_detector.dat\t model25.h5  model5.h5\n",
      "model0.h5\t\t\t model26.h5  model6.h5\n",
      "model10.h5\t\t\t model27.h5  model7.h5\n",
      "model11.h5\t\t\t model28.h5  model8.h5\n",
      "model12.h5\t\t\t model29.h5  model9.h5\n",
      "model13.h5\t\t\t model2.h5   model.weights.best.hdf5\n",
      "model14.h5\t\t\t model30.h5  sample_data\n",
      "model15.h5\t\t\t model31.h5  train_data.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gg2gf-mdm7zw"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1193
    },
    "colab_type": "code",
    "id": "2mkn1ldD-Rs_",
    "outputId": "d9c121e2-1eb9-4902-c80f-f63ffca5a0a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0620 14:19:03.077151 140452698228608 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file with ID 1U9ZwvoXBq3Ty9QW5bSj5MFbJCrW3W2zu\n",
      "Uploaded file with ID 17c1hGitWh7cVSVGrylgwdsTFzRlwh4e3\n",
      "Uploaded file with ID 1ezNo-h9ThwYA9K3gd8SZj5zRgtBkj9Qz\n",
      "Uploaded file with ID 1GD-YdIcY-AnHZATr0IjBpSqW9kHCo8cS\n",
      "Uploaded file with ID 1sJK4oYk_iEOfFYd7VDH1YLuojdRoz90E\n",
      "Uploaded file with ID 1PBok-lgHUPNns_XklpgefN4acsR25rgp\n",
      "Uploaded file with ID 13Mx-HYBRFi4INVYLsbDN062hZ__mBlBi\n",
      "Uploaded file with ID 1IoeFaqIsw4ywwT7nwnRE_VsvaFKk6adF\n",
      "Uploaded file with ID 1jZYAYHHSgg6gVUbcvgx23WAHq8mU6zaC\n",
      "Uploaded file with ID 13I_Wm32ie6ia_bPvDoYofunojUPVtnjI\n",
      "Uploaded file with ID 1s_7uobDiSER2VylVK35HL7j1wzoiueo5\n",
      "Uploaded file with ID 1RB0AdfJ0Vxly6cEpZKAjUajDQCP98m_s\n",
      "Uploaded file with ID 1A8fYc10058MQxA2Q7_-ZnQtSiiQXzJLZ\n",
      "Uploaded file with ID 1zeZqAU7YMzMD8Xekvgw_upl6m33usM99\n",
      "Uploaded file with ID 1bWh7LuIOoYGmr6BcFtq4dYU54CYAksN5\n",
      "Uploaded file with ID 1J1fvRzpNITWoV-g1ikau2AbwtU2cXK0U\n",
      "Uploaded file with ID 13owjkzUslMmhJ1GE7rQxxQvHxNtIYxUL\n",
      "Uploaded file with ID 1zRjDiN084BTk9RU8ZgsYkheD1whsWGuZ\n",
      "Uploaded file with ID 19eghaqi3PEj6Z3cyz1cXf01gXCcQgMZr\n",
      "Uploaded file with ID 1xQvcdbkvIgchTsNEQ57Hs5UOoShoGgxI\n",
      "Uploaded file with ID 1se8cXNIZ-HafvVAHxSot8VyD0ElNopuG\n",
      "Uploaded file with ID 1IJ31DV-48ngYnSCEmyNvAPKFUnI9wLGK\n",
      "Uploaded file with ID 1ETeck2Y18GXlM65UONdNy_hgP679Xrur\n",
      "Uploaded file with ID 1fGuugAbp8bEgcIwBy2tMOiyG6mgPzwrL\n",
      "Uploaded file with ID 1W3OzeHV5k19V23rxfRCezxGzElH2pWQi\n",
      "Uploaded file with ID 1kC5dT61TQ5HnPQtZZx6ZNl_O3VMR7fOT\n",
      "Uploaded file with ID 1h4mNSQApAZFUczM-UVbMgNzNWXX010WB\n",
      "Uploaded file with ID 1Ha6BS8WKYzaMjf-Unl_WUCpfOQEPomrA\n",
      "Uploaded file with ID 1DkaD_Hh2uC6u4efWnx063kWlk8weOmhV\n",
      "Uploaded file with ID 1zQ_cChBfn0HNH_dr6nsC2N4F6ovcgH6l\n",
      "Uploaded file with ID 1RZehQAWiZWUnAGwlmq-WWbLT-5li8ndM\n",
      "Uploaded file with ID 15SrhM8c6s4-DZjpQTwN7VvHoVzBBe_v0\n",
      "Uploaded file with ID 19ovcukn4T8VmNH0fGF8Pv76-n6aOGb4m\n",
      "Uploaded file with ID 12pJeVfo1tgN7uMc189f7zol0QlqCf4mF\n",
      "Uploaded file with ID 1aDSCNS8kKvAI7dtifjN5IH2btExiuTdJ\n",
      "Uploaded file with ID 1rWRPjqunhzD-6A9Fses-0KltI2tvZxCL\n",
      "Uploaded file with ID 1VcXxRYyeBrEjzvOsa8xZdllqpRU9qKmp\n",
      "Uploaded file with ID 1USdV8PBFbM62X9ap7HaiurgRQFB0uUvk\n",
      "Uploaded file with ID 1jz3FGYVH08oKqecKWP7c_AbQjHe9MZm2\n",
      "Uploaded file with ID 1463BoJ6Lj4CteWmERp8SUpA2ADygKyOe\n"
     ]
    }
   ],
   "source": [
    "model_ids=[]\n",
    "for i in range(40):\n",
    "  uploaded = drive.CreateFile({'title': 'model'+str(i)+'.h5'})\n",
    "  uploaded.SetContentFile('model'+str(i)+'.h5')\n",
    "  uploaded.Upload()\n",
    "  print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
    "  model_ids.append(uploaded.get('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VRqtdb5H-pk4",
    "outputId": "daac08cd-2a91-4020-b852-7c1bea7cca77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7WEylSI-uWi"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models.pkl', 'wb') as f:  \n",
    "    pickle.dump([model_ids], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-Zf8Ush-wqk"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "QfM7-XzD-zyc",
    "outputId": "cc5eddb3-fa97-427d-be54-464aa4c4a39d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2e5b8b1f-6769-4295-ab7d-573ee565fb3b\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-2e5b8b1f-6769-4295-ab7d-573ee565fb3b\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models-2.pkl to models-2.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'models-2.pkl': b'\\x80\\x03]q\\x00]q\\x01(X!\\x00\\x00\\x001sIPNRoWWqwW0ss5nuyqg1DKD7TBCOx6_q\\x02X!\\x00\\x00\\x001CPjmNc9_H3NqSbVJtHhfJuDkef-YkuLuq\\x03X!\\x00\\x00\\x001TaDjh4pvmk80m9H9DpTWJdvtuRe-1Zy4q\\x04X!\\x00\\x00\\x001yIIC6UcDQL78luRM1XvBHNhUPadi_SYEq\\x05X!\\x00\\x00\\x001jHFdrOCBK64Heq0C8papgZ2YS_fKsWOJq\\x06X!\\x00\\x00\\x001T5UofI1cnX_JOgfcdDXgy2Yeb0yad6Luq\\x07X!\\x00\\x00\\x001Xw9Yr4frQrpeK9sWrhpwSbnIEhFwJ-09q\\x08X!\\x00\\x00\\x001isawUtP1YPw-kRkypQ2zwjJ4mL57JPkdq\\tX!\\x00\\x00\\x001fUTqRGyfE7yVB-mYXR-R4OIp9FgPx-bFq\\nX!\\x00\\x00\\x001lK_X68f1FtnfFqprblbmWTERwrGGUOefq\\x0bX!\\x00\\x00\\x001wS-2xL2qQmyKVn0jd8ZMnYOOjSoVAhybq\\x0cX!\\x00\\x00\\x001XnRpRaNOXaqvRTxpLwJ-uouDlZdwdGZuq\\rX!\\x00\\x00\\x001PmJR7pmwOVUiTc-1r_Ll3bDPJ2lIZkpqq\\x0eX!\\x00\\x00\\x001wK9CedTZ1umbpXp_XYM0tvHTjF_bzDuyq\\x0fX!\\x00\\x00\\x001P5_sXfsOw0ZcvceiLJw5q25uTqt8w3Ynq\\x10X!\\x00\\x00\\x00154WOUyU0RXPCkRe4NuRPeZHGV6RCEEjmq\\x11X!\\x00\\x00\\x0013Gk3v7n0azs1WM6BEDN0MubY0QRBjVNZq\\x12X!\\x00\\x00\\x001nmLgMQMzd1b377Qk4UpK5lN0QCfQGC0wq\\x13X!\\x00\\x00\\x001sipeNw-mj5uNB8mWIHli7xAo_GtuXl3oq\\x14X!\\x00\\x00\\x001BOWr_yyI4aF_3bfjPBPYQMxyPm7qZb2vq\\x15X!\\x00\\x00\\x001R6EnBniRK6uDKh5Vadit858o2YxQuHcEq\\x16X!\\x00\\x00\\x001mhBhU6C1-cp8lbGPFGWQuJTXkxJJa1e2q\\x17X!\\x00\\x00\\x001ECaEWqxcOsSYh-n-VmKS5YnubwWQ9ofjq\\x18X!\\x00\\x00\\x001MZMkwZEgENChCvWfX6tRnUXq6WY1Mb-Jq\\x19X!\\x00\\x00\\x001g4iRK1vo-YrI5mTUXIF2oKrIG3FXv2F_q\\x1aX!\\x00\\x00\\x0010GCcMWmoTd7fOY_TctyC6uyuAkHEEjaqq\\x1bX!\\x00\\x00\\x001Svxgpw7ODiNDFqJC3cTplncX14hom7f-q\\x1cX!\\x00\\x00\\x001twCC3Xa_ND-HBnY4JQxcHICaIO8r_Odzq\\x1dX!\\x00\\x00\\x001MfrBQkQ8A95g29mMbhy3dl10ziUd36LYq\\x1eX!\\x00\\x00\\x001MAuWPgsVdr4zOl-hJgM0GAm5NG572KYHq\\x1fX!\\x00\\x00\\x001llGcg2_z9gpo6oFBqQJ825ck8uX7hTbLq X!\\x00\\x00\\x001ZR3OEIKGl1oqCn-G71N336CqiIoDd2GYq!X!\\x00\\x00\\x0012NDReQhLlzH6ucO73VjgvCRQGx5IK-naq\"X!\\x00\\x00\\x001o-_hCBSv8GFkFfNlrrRAH7jqoWEvXGCbq#X!\\x00\\x00\\x001US7Qb8UprtLhu6GM4fs_gcHs0jLc8kHtq$X!\\x00\\x00\\x001HvSaYa8snKlq5M24jEdyxveD45Qt0Aoaq%X!\\x00\\x00\\x001h86MLbnQ6LpBfPvMNZuFhvMJcklXbX41q&X!\\x00\\x00\\x001WbcMEsy9XaNHSeNhoK8iGTKj2uy67Dhoq\\'X!\\x00\\x00\\x001ohTc6BK-4aGhArC-3LcmJea_KTmrbaQ7q(X!\\x00\\x00\\x001g_XUsFTrt6IDC69GkOSi5qkJtP002r8lq)ea.'}"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qfpwvsfk_NbI"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('models.pkl','rb') as f: \n",
    "   model_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xbnvPuX_RZh"
   },
   "outputs": [],
   "source": [
    "model_ids=model_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tLYG0Nqa_TBs",
    "outputId": "bd58f0a3-1926-47c2-bc1e-2a00b65465f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1U9ZwvoXBq3Ty9QW5bSj5MFbJCrW3W2zu'"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nD_m3YP4_YTw"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XFKugDG_aw8"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1193
    },
    "colab_type": "code",
    "id": "FTGF08pO_hI8",
    "outputId": "4a668051-7723-4f1c-907b-e9dd96cefcd1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0620 14:26:16.278231 140452698228608 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 0\n",
      "file: 1\n",
      "file: 2\n",
      "file: 3\n",
      "file: 4\n",
      "file: 5\n",
      "file: 6\n",
      "file: 7\n",
      "file: 8\n",
      "file: 9\n",
      "file: 10\n",
      "file: 11\n",
      "file: 12\n",
      "file: 13\n",
      "file: 14\n",
      "file: 15\n",
      "file: 16\n",
      "file: 17\n",
      "file: 18\n",
      "file: 19\n",
      "file: 20\n",
      "file: 21\n",
      "file: 22\n",
      "file: 23\n",
      "file: 24\n",
      "file: 25\n",
      "file: 26\n",
      "file: 27\n",
      "file: 28\n",
      "file: 29\n",
      "file: 30\n",
      "file: 31\n",
      "file: 32\n",
      "file: 33\n",
      "file: 34\n",
      "file: 35\n",
      "file: 36\n",
      "file: 37\n",
      "file: 38\n",
      "file: 39\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "  download = drive.CreateFile({'id': model_ids[i]})\n",
    "  download.GetContentFile('model'+str(i)+'.h5')\n",
    "  print(\"file:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "N6iY_dLx_sw8",
    "outputId": "c69c5a64-17c3-49ba-b291-a13c3e904b6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 202599 entries, 0 to 202598\n",
      "Data columns (total 41 columns):\n",
      "image_id               202599 non-null object\n",
      "5_o_Clock_Shadow       202599 non-null int64\n",
      "Arched_Eyebrows        202599 non-null int64\n",
      "Attractive             202599 non-null int64\n",
      "Bags_Under_Eyes        202599 non-null int64\n",
      "Bald                   202599 non-null int64\n",
      "Bangs                  202599 non-null int64\n",
      "Big_Lips               202599 non-null int64\n",
      "Big_Nose               202599 non-null int64\n",
      "Black_Hair             202599 non-null int64\n",
      "Blond_Hair             202599 non-null int64\n",
      "Blurry                 202599 non-null int64\n",
      "Brown_Hair             202599 non-null int64\n",
      "Bushy_Eyebrows         202599 non-null int64\n",
      "Chubby                 202599 non-null int64\n",
      "Double_Chin            202599 non-null int64\n",
      "Eyeglasses             202599 non-null int64\n",
      "Goatee                 202599 non-null int64\n",
      "Gray_Hair              202599 non-null int64\n",
      "Heavy_Makeup           202599 non-null int64\n",
      "High_Cheekbones        202599 non-null int64\n",
      "Male                   202599 non-null int64\n",
      "Mouth_Slightly_Open    202599 non-null int64\n",
      "Mustache               202599 non-null int64\n",
      "Narrow_Eyes            202599 non-null int64\n",
      "No_Beard               202599 non-null int64\n",
      "Oval_Face              202599 non-null int64\n",
      "Pale_Skin              202599 non-null int64\n",
      "Pointy_Nose            202599 non-null int64\n",
      "Receding_Hairline      202599 non-null int64\n",
      "Rosy_Cheeks            202599 non-null int64\n",
      "Sideburns              202599 non-null int64\n",
      "Smiling                202599 non-null int64\n",
      "Straight_Hair          202599 non-null int64\n",
      "Wavy_Hair              202599 non-null int64\n",
      "Wearing_Earrings       202599 non-null int64\n",
      "Wearing_Hat            202599 non-null int64\n",
      "Wearing_Lipstick       202599 non-null int64\n",
      "Wearing_Necklace       202599 non-null int64\n",
      "Wearing_Necktie        202599 non-null int64\n",
      "Young                  202599 non-null int64\n",
      "dtypes: int64(40), object(1)\n",
      "memory usage: 63.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "data=pd.read_csv('/content/list_attr_celeba.csv')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQAR3y4f_xuA"
   },
   "outputs": [],
   "source": [
    "col_list=data.columns.tolist()\n",
    "col_list.remove('image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Yb5OrYt_1aX"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import os.path as path\n",
    "from scipy import misc\n",
    "\n",
    "IMAGE_PATH = '/content/img_align_celeba'\n",
    "file_paths = glob.glob(path.join(IMAGE_PATH, '00*.jpg'))\n",
    "\n",
    "images = [imageio.imread(path) for path in file_paths]\n",
    "images = images[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qHElnbLC_8R5",
    "outputId": "59e088af-db08-46bf-d29f-3cdac02883b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNENYKoB__Bg"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "new_img=[]\n",
    "for i in range(len(images)):\n",
    "    img=cv2.resize(images[i],(64,64))\n",
    "    new_img.append(img)\n",
    "    \n",
    "     \n",
    "images=np.asarray(new_img)\n",
    "images=images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4777
    },
    "colab_type": "code",
    "id": "LEGNafiOAL-i",
    "outputId": "31ea3e85-a8a7-4255-ffab-2c48f0a5e91f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Attribute:  0\n",
      "5_o_Clock_Shadow\n",
      "[0. 1.]\n",
      "[0.29292825 0.7070717 ]\n",
      "Acc: 83.97\n",
      "\n",
      "\n",
      "Attribute:  1\n",
      "Arched_Eyebrows\n",
      "[1. 0.]\n",
      "[0.9869083  0.01309167]\n",
      "Acc: 73.8\n",
      "\n",
      "\n",
      "Attribute:  2\n",
      "Attractive\n",
      "[0. 1.]\n",
      "[0.8584994  0.14150058]\n",
      "Acc: 76.0\n",
      "\n",
      "\n",
      "Attribute:  3\n",
      "Bags_Under_Eyes\n",
      "[1. 0.]\n",
      "[0.41627172 0.5837283 ]\n",
      "Acc: 73.14\n",
      "\n",
      "\n",
      "Attribute:  4\n",
      "Bald\n",
      "[1. 0.]\n",
      "[0.87305784 0.12694214]\n",
      "Acc: 92.72\n",
      "\n",
      "\n",
      "Attribute:  5\n",
      "Bangs\n",
      "[1. 0.]\n",
      "[0.9976549  0.00234506]\n",
      "Acc: 89.82\n",
      "\n",
      "\n",
      "Attribute:  6\n",
      "Big_Lips\n",
      "[1. 0.]\n",
      "[0.8364755  0.16352445]\n",
      "Acc: 57.28\n",
      "\n",
      "\n",
      "Attribute:  7\n",
      "Big_Nose\n",
      "[1. 0.]\n",
      "[0.19274679 0.8072532 ]\n",
      "Acc: 69.24\n",
      "\n",
      "\n",
      "Attribute:  8\n",
      "Black_Hair\n",
      "[1. 0.]\n",
      "[0.8987297  0.10127027]\n",
      "Acc: 83.25\n",
      "\n",
      "\n",
      "Attribute:  9\n",
      "Blond_Hair\n",
      "[1. 0.]\n",
      "[0.9986337  0.00136624]\n",
      "Acc: 91.47\n",
      "\n",
      "\n",
      "Attribute:  10\n",
      "Blurry\n",
      "[1. 0.]\n",
      "[0.94748425 0.05251567]\n",
      "Acc: 86.00999999999999\n",
      "\n",
      "\n",
      "Attribute:  11\n",
      "Brown_Hair\n",
      "[0. 1.]\n",
      "[0.8121215  0.18787849]\n",
      "Acc: 79.09\n",
      "\n",
      "\n",
      "Attribute:  12\n",
      "Bushy_Eyebrows\n",
      "[1. 0.]\n",
      "[0.9757456  0.02425442]\n",
      "Acc: 82.37\n",
      "\n",
      "\n",
      "Attribute:  13\n",
      "Chubby\n",
      "[1. 0.]\n",
      "[0.12706758 0.8729324 ]\n",
      "Acc: 83.41\n",
      "\n",
      "\n",
      "Attribute:  14\n",
      "Double_Chin\n",
      "[1. 0.]\n",
      "[0.07882188 0.9211781 ]\n",
      "Acc: 85.39\n",
      "\n",
      "\n",
      "Attribute:  15\n",
      "Eyeglasses\n",
      "[1. 0.]\n",
      "[0.97085446 0.02914547]\n",
      "Acc: 96.37\n",
      "\n",
      "\n",
      "Attribute:  16\n",
      "Goatee\n",
      "[1. 0.]\n",
      "[0.3692397 0.6307603]\n",
      "Acc: 88.74\n",
      "\n",
      "\n",
      "Attribute:  17\n",
      "Gray_Hair\n",
      "[1. 0.]\n",
      "[0.27562755 0.72437245]\n",
      "Acc: 90.99000000000001\n",
      "\n",
      "\n",
      "Attribute:  18\n",
      "Heavy_Makeup\n",
      "[1. 0.]\n",
      "[9.9998963e-01 1.0335579e-05]\n",
      "Acc: 85.99\n",
      "\n",
      "\n",
      "Attribute:  19\n",
      "High_Cheekbones\n",
      "[0. 1.]\n",
      "[0.14917064 0.8508294 ]\n",
      "Acc: 79.17\n",
      "\n",
      "\n",
      "Attribute:  20\n",
      "Male\n",
      "[0. 1.]\n",
      "[4.9599417e-05 9.9995041e-01]\n",
      "Acc: 93.05\n",
      "\n",
      "\n",
      "Attribute:  21\n",
      "Mouth_Slightly_Open\n",
      "[0. 1.]\n",
      "[2.2497251e-04 9.9977499e-01]\n",
      "Acc: 83.77\n",
      "\n",
      "\n",
      "Attribute:  22\n",
      "Mustache\n",
      "[1. 0.]\n",
      "[0.07911535 0.92088467]\n",
      "Acc: 86.86\n",
      "\n",
      "\n",
      "Attribute:  23\n",
      "Narrow_Eyes\n",
      "[1. 0.]\n",
      "[0.45693782 0.54306215]\n",
      "Acc: 72.96000000000001\n",
      "\n",
      "\n",
      "Attribute:  24\n",
      "No_Beard\n",
      "[1. 0.]\n",
      "[0.59444493 0.40555507]\n",
      "Acc: 85.24000000000001\n",
      "\n",
      "\n",
      "Attribute:  25\n",
      "Oval_Face\n",
      "[1. 0.]\n",
      "[0.72357565 0.27642432]\n",
      "Acc: 61.1\n",
      "\n",
      "\n",
      "Attribute:  26\n",
      "Pale_Skin\n",
      "[1. 0.]\n",
      "[0.9968166  0.00318351]\n",
      "Acc: 93.28\n",
      "\n",
      "\n",
      "Attribute:  27\n",
      "Pointy_Nose\n",
      "[1. 0.]\n",
      "[0.54850066 0.45149934]\n",
      "Acc: 60.36\n",
      "\n",
      "\n",
      "Attribute:  28\n",
      "Receding_Hairline\n",
      "[1. 0.]\n",
      "[0.12185006 0.8781499 ]\n",
      "Acc: 79.02\n",
      "\n",
      "\n",
      "Attribute:  29\n",
      "Rosy_Cheeks\n",
      "[1. 0.]\n",
      "[0.9224799  0.07752004]\n",
      "Acc: 87.72999999999999\n",
      "\n",
      "\n",
      "Attribute:  30\n",
      "Sideburns\n",
      "[1. 0.]\n",
      "[0.9029764  0.09702361]\n",
      "Acc: 89.89\n",
      "\n",
      "\n",
      "Attribute:  31\n",
      "Smiling\n",
      "[0. 1.]\n",
      "[0.2482781  0.75172186]\n",
      "Acc: 84.5\n",
      "\n",
      "\n",
      "Attribute:  32\n",
      "Straight_Hair\n",
      "[0. 1.]\n",
      "[0.18661885 0.81338114]\n",
      "Acc: 58.06\n",
      "\n",
      "\n",
      "Attribute:  33\n",
      "Wavy_Hair\n",
      "[1. 0.]\n",
      "[0.8626941 0.1373059]\n",
      "Acc: 71.65\n",
      "\n",
      "\n",
      "Attribute:  34\n",
      "Wearing_Earrings\n",
      "[1. 0.]\n",
      "[0.9815507 0.0184493]\n",
      "Acc: 70.86\n",
      "\n",
      "\n",
      "Attribute:  35\n",
      "Wearing_Hat\n",
      "[1. 0.]\n",
      "[9.9947220e-01 5.2776356e-04]\n",
      "Acc: 95.28999999999999\n",
      "\n",
      "\n",
      "Attribute:  36\n",
      "Wearing_Lipstick\n",
      "[1. 0.]\n",
      "[9.999467e-01 5.324028e-05]\n",
      "Acc: 89.03\n",
      "\n",
      "\n",
      "Attribute:  37\n",
      "Wearing_Necklace\n",
      "[1. 0.]\n",
      "[9.9940932e-01 5.9068756e-04]\n",
      "Acc: 67.24\n",
      "\n",
      "\n",
      "Attribute:  38\n",
      "Wearing_Necktie\n",
      "[1. 0.]\n",
      "[0.02881022 0.9711898 ]\n",
      "Acc: 88.72\n",
      "\n",
      "\n",
      "Attribute:  39\n",
      "Young\n",
      "[0. 1.]\n",
      "[0.6982317  0.30176827]\n",
      "Acc: 79.35\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "s=0\n",
    "t=0\n",
    "acc=[]\n",
    "for j in range(40):\n",
    "  print(\"\\n\\nAttribute: \",j)\n",
    "  print(col_list[j])\n",
    "  labels=[]\n",
    "  for i in range(len(images)):\n",
    "    filename = path.basename(file_paths[i])\n",
    "    index=int(filename.split(\".\")[0])\n",
    "    if data[col_list[j]][index-1]==1:\n",
    "      labels.append(1)\n",
    "    else:\n",
    "      labels.append(0)\n",
    "  labels=np.asarray(labels)\n",
    "  labels=np_utils.to_categorical(labels,2)\n",
    "  model = load_model('model'+str(j)+'.h5')\n",
    "  print(labels[0])\n",
    "  preds=model.predict(images)\n",
    "  print(preds[0])\n",
    "  preds=np.round(preds, 2)\n",
    "  preds[preds>=0.6] = 1\n",
    "  preds[preds<0.6] = 0\n",
    "  #예측값(preds)가 몇퍼센트 이상이면 1 (해당 특징을 갖고 있다고 판정) 아니면 0 (해당 특징을 갖고 있지 않다고 판정)할지 조정할 수 있다. \n",
    "  score=np.sum(preds==labels)/(preds==labels).size\n",
    "  print(\"Acc:\", score*100) \n",
    "  acc.append(score*100)\n",
    "  s=s+np.sum(preds==labels)\n",
    "  t=t+(preds==labels).size\n",
    "# 시간이 다소 소요된다. (약 115-20분 정도)\n",
    "#카테고리별 정확도와, 전체 평균 정확도를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gT5lBcCwJAHL",
    "outputId": "3801930c-ae65-4e41-c3ff-ff4d1d0ce100"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.811545"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"전체 평균 avg : \", s/t)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AllTrainFinall01.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
